<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>K-Means算法实现</title>
    <url>/2020/10/27/K-Means%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>本篇博客的主要内容有：</p>
<ol>
<li>K-means算法</li>
<li>python代码实现<ul>
<li>任意选择K(类别)聚类</li>
<li>选取最佳的k,画出聚类后的图像分布</li>
<li>计算准则函数J,画出J的变化图像</li>
<li>完整python代码</li>
</ul>
</li>
</ol>
<hr>
<a id="more"></a>
<p>这是模式识别课的作业</p>
<blockquote>
<p>试用K—均值法对如下模式分布进行聚类分析。编程实现，编程语言不限。<br>    {x1(0, 0), x2(3,8), x3(2,2), x4(1,1),<br>    x5(5,3), x6(4,8), x7(6,3), x8(5,4),<br>    x9(6,4), x10(7,5)}</p>
</blockquote>
<h2 id="K-Means算法"><a href="#K-Means算法" class="headerlink" title="K-Means算法"></a>K-Means算法</h2><p>K均值是一种动态巨累法<br><strong>动态聚类法</strong>先粗略地进行预分类，然后再逐步调整，直到把类分得比较合理为止，这种方法计算量较小、使用内存量小、方法简单,适用于大样本的聚类分析。<br>K均值算法的思想是首先任意选取K个聚类中心，按最小距离原则将各模式分配到K类的某一类,再不断计算聚类中心和调整各模式的类别,最终使各模式到其判属类别中心的距离平方之和最小。<br>具体步骤:</p>
<ol>
<li>任意选K个聚类中心(可以随机,或者直接选样本的前K个,后面会动态调整)</li>
<li>遍历所有样本,将每个样本按照最小距离原则分到K类中的一类</li>
<li>更新聚类中心,具体为将每个类别的聚类中心改为该类中所有样本的均值</li>
<li>一直重复上述过程,直到聚类中心保持不变。</li>
</ol>
<p>这里有个问题是,为什么<strong>聚类中心为什么要选择样本的均值</strong>,在下面的准则函数中将会给出推导过程。<br>所以K均值算法的实现最重要的就是计算距离和均值。</p>
<h2 id="准则函数定义"><a href="#准则函数定义" class="headerlink" title="准则函数定义"></a>准则函数定义</h2><ul>
<li><p>对于第j个类别$S_{j}$,有$N_{j}$个样本,聚类中心为$Z_{j}$</p>
<p>  $J_{j} = \sum_{i=1}^{N_{j}}\parallel X_{i} - Z{j} \parallel ^{2}$</p>
<p>  (这里应该是二范数的平方,markdown公式和博客代码不是很兼容的原因导致只是显示了一条竖线)</p>
</li>
<li><p>对于所有K个类别的准则函数就是把每个类别的求和就行</p>
<p>  由上面的准则函数,可以得到,对于某一个聚类$j$,应有:</p>
<p>  $\frac{\partial J_{j}}{\partial Z_{j}} = 0$</p>
<p>  那么有:</p>
<p>  $\frac{\partial \sum_{i=1}^{N_{j}}\parallel X_{i} - Z{j} \parallel ^{2}}{\partial Z_{j}} = \frac{\partial \sum_{i=1}^{N_{j}}(X_{i}-Z_{j})^{T}(X_{i}-Z_{j}) }{\partial Z_{j}} = 0$</p>
<p>  可以解得:<br>  $Z_{j}= \frac{1}{N_{j}}\sum_{i=1}^{N_{j}}X_{i},X_{i} \in S_{j}$</p>
</li>
</ul>
<p>所以 为了使得准则函数最小,$S_{j}$类的聚类中心应选为<strong>该类样本的均值</strong></p>
<p>下面是作业的python实现。</p>
<h2 id="K-Means的python实现"><a href="#K-Means的python实现" class="headerlink" title="K-Means的python实现"></a>K-Means的python实现</h2><ol>
<li><p>首先,K均值算法的K我们是没有办法事先预知的,所以,我先画出了图像的分布情况。<br>下面是原始数据,将其保存在txt文件中:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0,0</span><br><span class="line">3,8</span><br><span class="line">2,2</span><br><span class="line">1,1</span><br><span class="line">5,3</span><br><span class="line">4,8</span><br><span class="line">6,3</span><br><span class="line">5,4</span><br><span class="line">6,4</span><br><span class="line">7,5</span><br></pre></td></tr></table></figure>
<p>先将其读入,并且画出图像</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">path &#x3D;  &#39;data.txt&#39;</span><br><span class="line">data &#x3D; pd.read_csv(path, header&#x3D;None, names&#x3D;[&#39;x1&#39;, &#39;x2&#39;])</span><br></pre></td></tr></table></figure>
<p><img src="/images/3 kmeans/data.png" width="50%" height="50%"></p>
</li>
<li><p>计算距离</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def calDist(X,center):</span><br><span class="line">    return np.linalg.norm(X - center,2,1)</span><br></pre></td></tr></table></figure>
<p>这里直接返回了二范数的距离(平方再开根号)</p>
</li>
<li><p>K-means算法的实现</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def kmeans(X,k):</span><br><span class="line">#@ return centers,clusters[每个聚类中心,每个样本所属的类别]</span><br><span class="line">    centers &#x3D; X[:k,:]</span><br><span class="line">    newCenters &#x3D; np.zeros(centers.shape)</span><br><span class="line">    clusters &#x3D; np.zeros(len(X))</span><br><span class="line">    while True: #当聚类中心不再变化时算法停止</span><br><span class="line"></span><br><span class="line">        #计算每个点到所有中心的距离</span><br><span class="line">        for i in range(len(X)):</span><br><span class="line">            d &#x3D; calDist(X[i], centers)</span><br><span class="line">            cluster &#x3D; np.argmin(d) </span><br><span class="line">            clusters[i] &#x3D; cluster #第i个样本的类别设定为到所有聚类中心最近的那个</span><br><span class="line">        #遍历k个中心,分别找出以该聚类中心的所有点并且求均值作为新的聚类中心</span><br><span class="line">        for i in range(k):</span><br><span class="line">            points &#x3D; [X[j] for j in range(len(X)) if clusters[j] &#x3D;&#x3D; i]</span><br><span class="line">            newCenters[i] &#x3D; np.mean(points, axis&#x3D;0)</span><br><span class="line">        if (centers &#x3D;&#x3D; newCenters).all():#当聚类中心不再变化时,终止循环</span><br><span class="line">            break </span><br><span class="line">        else:</span><br><span class="line">            centers &#x3D; newCenters</span><br><span class="line">    return centers,clusters</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="K-3的代码分析"><a href="#K-3的代码分析" class="headerlink" title="K = 3的代码分析"></a>K = 3的代码分析</h2><p>由于我们之前已经画出了图像,可以很自然的得到他应该分为3个类别,使用K-means算法,并画出图像<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">centers,clusters &#x3D; kmeans(X,k)</span><br><span class="line">colors &#x3D; [&#39;r&#39;, &#39;g&#39;, &#39;b&#39;]</span><br><span class="line">fig, ax &#x3D; plt.subplots()</span><br><span class="line">for i in range(k):</span><br><span class="line">    points &#x3D; np.array([X[j] for j in range(len(X)) if clusters[j] &#x3D;&#x3D; i])</span><br><span class="line">    #上面是3*1*2的array,先用flatten变为1*len(X)维的,再reshape</span><br><span class="line">    points &#x3D; points.flatten()</span><br><span class="line">    points &#x3D; np.reshape(points,(int(len(points)&#x2F;2),2))</span><br><span class="line">    print(points[:,0],points[:,1])      </span><br><span class="line">    ax.scatter(points[:,0],points[:,1], s&#x3D;20, c&#x3D;colors[i])</span><br><span class="line">ax.scatter(newCenters[:, 0], newCenters[:, 1], marker&#x3D;&#39;*&#39;, s&#x3D;30, c&#x3D;&#39;y&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br>下面是K-means算法后的结果,其中五角星是聚类中心<br> <img src="/images/3 kmeans/k3.png" width="50%" height="50%"><br>由图可知,分成了三类,每个聚类中心分别为<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[1.  1. ] </span><br><span class="line"> [3.5 8. ] </span><br><span class="line"> [5.8 3.8]]</span><br></pre></td></tr></table></figure></p>
<h2 id="取K不同值时J的变化"><a href="#取K不同值时J的变化" class="headerlink" title="取K不同值时J的变化"></a>取K不同值时J的变化</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">J &#x3D; np.zeros([11,1])</span><br><span class="line">with warnings.catch_warnings():</span><br><span class="line">    warnings.filterwarnings(&#39;error&#39;)</span><br><span class="line">    #依次计算出J[k](准则函数)</span><br><span class="line">    for k in range(1,11):</span><br><span class="line">        try:</span><br><span class="line">            centers,clusters &#x3D; kmeans(X,k)</span><br><span class="line">            for i in range(len(X)):</span><br><span class="line">                d &#x3D; calDist(X[i],centers[int(clusters[i])])</span><br><span class="line">                J[k] &#x3D; J[k] + (d**2)              </span><br><span class="line">        except:</span><br><span class="line">            print(&quot;None&quot;)</span><br><span class="line">            </span><br><span class="line">fig, ax &#x3D; plt.subplots(figsize&#x3D;(12,8))</span><br><span class="line">ax.plot(range(1,11), J[1:], &#39;b&#39;)</span><br><span class="line"></span><br><span class="line">#下面是为了标出每个点的坐标</span><br><span class="line">for i_x, i_y in zip(range(1,11), J[1:]):</span><br><span class="line">    plt.text(i_x, i_y, &#39;(&#123;&#125;, &#123;&#125;)&#39;.format(i_x, i_y))</span><br><span class="line">ax.scatter(range(1,11),J[1:])</span><br><span class="line">ax.scatter(3,J[3:],&#39;r&#39;)</span><br><span class="line">ax.set_xlabel(&#39;k&#39;)</span><br><span class="line">ax.set_ylabel(&#39;J&#39;)</span><br><span class="line">ax.set_title(&#39;J vs k&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>下面是取不同k值的j的变化.这里的J是距离平方的和</p>
<p> <img src="/images/3 kmeans/j vs k.png" width="50%" height="50%"></p>
<p>一般来说,如果样本集的合理聚类数为K类，当类别数从1增加到K时准则函数迅速减小，当类别数超过K时，准则函数虽然继续减少但会呈现平缓趋势。 所以<strong>K一般会选择在拐点</strong><br>上图中K=3的时候是比较合适的点(在K&gt;3之后下降缓慢)。</p>
<p><strong>K均值的结果受到所选聚类中心的个数和其初始位置的影响较大,实际应用中需要试探不同的K值和选择不同的聚类中心起始值。(这里我直接取了前K个样本)</strong></p>
<h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import warnings</span><br><span class="line"></span><br><span class="line">def calDist(X,center):</span><br><span class="line">    return np.linalg.norm(X - center,2,1)</span><br><span class="line"></span><br><span class="line">def kmeans(X,k):</span><br><span class="line">    np.seterr(invalid&#x3D;&#39;ignore&#39;)</span><br><span class="line">    centers &#x3D; X[:k,:]</span><br><span class="line">    newCenters &#x3D; np.zeros(centers.shape)</span><br><span class="line">    clusters &#x3D; np.zeros(len(X))</span><br><span class="line">    while True:</span><br><span class="line">        for i in range(len(X)):</span><br><span class="line">            d &#x3D; calDist(X[i], centers)</span><br><span class="line">            cluster &#x3D; np.argmin(d) </span><br><span class="line">            clusters[i] &#x3D; cluster</span><br><span class="line">        for i in range(k):</span><br><span class="line">            points &#x3D; [X[j] for j in range(len(X)) if clusters[j] &#x3D;&#x3D; i]</span><br><span class="line">            newCenters[i] &#x3D; np.mean(points, axis&#x3D;0)</span><br><span class="line">        if (centers &#x3D;&#x3D; newCenters).all():</span><br><span class="line">            break </span><br><span class="line">        else:</span><br><span class="line">            centers &#x3D; newCenters</span><br><span class="line">    return centers,clusters</span><br><span class="line"></span><br><span class="line">def plotPoints(X,centers,clusters,k):</span><br><span class="line">    colors &#x3D; [&#39;r&#39;, &#39;g&#39;, &#39;b&#39;, &#39;y&#39;, &#39;c&#39;, &#39;m&#39;]</span><br><span class="line">    fig, ax &#x3D; plt.subplots()</span><br><span class="line">    for i in range(k):</span><br><span class="line">        points &#x3D; np.array([X[j] for j in range(len(X)) if clusters[j] &#x3D;&#x3D; i])</span><br><span class="line">        points &#x3D; points.flatten()</span><br><span class="line">        points &#x3D; np.reshape(points,(int(len(points)&#x2F;2),2))</span><br><span class="line">        print(points[:,0],points[:,1])      </span><br><span class="line">        ax.scatter(points[:,0],points[:,1], s&#x3D;7, c&#x3D;colors[i])</span><br><span class="line">    ax.scatter(centers[:, 0], centers[:, 1], marker&#x3D;&#39;*&#39;, s&#x3D;10, c&#x3D;&#39;y&#39;)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    path &#x3D;  &#39;data.txt&#39;</span><br><span class="line">    data &#x3D; pd.read_csv(path, header&#x3D;None, names&#x3D;[&#39;x1&#39;, &#39;x2&#39;])</span><br><span class="line">    X &#x3D; data</span><br><span class="line">    X &#x3D; np.matrix(X.values)</span><br><span class="line">    J &#x3D; np.zeros([11,1])</span><br><span class="line">    with warnings.catch_warnings():</span><br><span class="line">        warnings.filterwarnings(&#39;error&#39;)</span><br><span class="line">        for k in range(1,11):</span><br><span class="line">            try:</span><br><span class="line">                centers,clusters &#x3D; kmeans(X,k)</span><br><span class="line">                for i in range(len(X)):</span><br><span class="line">                    d &#x3D; calDist(X[i],centers[int(clusters[i])])</span><br><span class="line">                    J[k] &#x3D; J[k] + (d**2)              </span><br><span class="line">            except:</span><br><span class="line">                print(&quot;None&quot;)      </span><br><span class="line">    fig, ax &#x3D; plt.subplots(figsize&#x3D;(12,8))</span><br><span class="line">    ax.plot(range(1,11), J[1:], &#39;b&#39;)</span><br><span class="line">    # ax.scatter(3,J[3],&#39;r&#39;)</span><br><span class="line">    for i_x, i_y in zip(range(1,11), J[1:]):</span><br><span class="line">        plt.text(i_x, i_y, &#39;(&#123;&#125;, &#123;&#125;)&#39;.format(i_x, i_y[0]))</span><br><span class="line">    ax.scatter(range(1,11),J[1:])</span><br><span class="line">    ax.set_xlabel(&#39;k&#39;)</span><br><span class="line">    ax.set_ylabel(&#39;J&#39;)</span><br><span class="line">    ax.set_title(&#39;J vs k&#39;)</span><br><span class="line">    plt.show()</span><br><span class="line">main()</span><br></pre></td></tr></table></figure>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a href="https://www.cnblogs.com/shenfeng/p/kmeans_demo.html" target="_blank" rel="noopener">K-Means算法的Python实现</a></li>
<li>模式识别课的PPT</li>
</ol>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title>NG机器学习EX1(线性回归)</title>
    <url>/2020/10/24/NG%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0EX1(%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92)/</url>
    <content><![CDATA[<p>本篇博客的主要内容有：</p>
<ol>
<li>python实现的ex1</li>
<li>梯度下降<ul>
<li>特征缩放</li>
<li>学习率选取</li>
</ul>
</li>
<li>正规方程</li>
</ol>
<hr>
<a id="more"></a>
<p>这篇博客的主要来源是吴恩达老师的机器学习第一周、第二周的学习课程。其中部分内容参考自<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">黄海广博士的笔记内容</a>,先根据作业完成了matlab代码,并且学习将它用python来实现(博客中无matlab实现)。在学习的过程中我结合吴恩达老师的上课内容和matlab代码修改了一部分代码(个人认为有点小错误以及可以优化的地方)。</p>
<h2 id="读入数据并且可视化数据"><a href="#读入数据并且可视化数据" class="headerlink" title="读入数据并且可视化数据"></a>读入数据并且可视化数据</h2><p>第一部分的数据由两列组成,第一列是population(城市人口),第二列是profit(食物车的利润),在这里是2D数据,我们可以可视化它。</p>
<ol>
<li>读入数据<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">path &#x3D;  &#39;ex1data1.txt&#39;</span><br><span class="line">data &#x3D; pd.read_csv(path, header&#x3D;None, names&#x3D;[&#39;Population&#39;, &#39;Profit&#39;])</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure></li>
<li>画出数据的分布<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data.plot(kind&#x3D;&#39;scatter&#39;, x&#x3D;&#39;Population&#39;, y&#x3D;&#39;Profit&#39;, figsize&#x3D;(12,8))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img src="/images/data1.png" width="50%" height="50%"> </li>
</ol>
<h2 id="梯度下降知识点回顾"><a href="#梯度下降知识点回顾" class="headerlink" title="梯度下降知识点回顾"></a>梯度下降知识点回顾</h2><p> 作业中把梯度下降方法分成了两个函数,一个是单变量的线性回归,一个是多变量的线性回归,这里是直接实现的是多变量的线性回归。</p>
<p> 代价函数为:</p>
<ul>
<li>单变量<br>  $J\left( {\theta_{0}},{\theta_{1}}…{\theta_{n}} \right)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta }(x^{(i)})-y^{(i)})^{2}$</li>
<li>多变量<br>  $J(\theta ) = \frac{1}{2m}(X\theta -y)^{T}(X\theta -y)$<br>  s.t. $h_{\theta}(x)=\theta^{T}X={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+…+{\theta_{n}}{x_{n}}$<br>  对$J$求导后,得到梯度下降更新算法<br><img src="/images/GradientDescent.png" width="50%" height="50%"><br><strong>注意这里的每个$\theta_{j}$都需要同时更新</strong><br>每次迭代计算后,每个$\theta_{j}$会趋于最优值,进而达到最低的cost,从而达到收敛</li>
</ul>
<h2 id="设置X-y-theta并转为numpy矩阵"><a href="#设置X-y-theta并转为numpy矩阵" class="headerlink" title="设置X,y,theta并转为numpy矩阵"></a>设置X,y,theta并转为numpy矩阵</h2><p>实现过程中,我们设$X$为列向量,为了更方便的计算(向量化),考虑$\theta_{0}$,在$X$中插入每个元素为1的列向量,这样我们就可以轻易的使用代码来实现$h_{\theta}(x)=X \cdot\theta^{T}$来计算$h_{\theta}(x)$了<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data.insert(0,&#39;Ones&#39;,1)</span><br><span class="line">cols &#x3D; data.shape[1]</span><br><span class="line">X &#x3D; data.iloc[:,0:cols-1] # X是前cols-1列</span><br><span class="line">y &#x3D; data.iloc[:,cols-1:cols]#y是最后一列</span><br><span class="line">#将X y theta转换为numpy矩阵</span><br><span class="line">X &#x3D; np.matrix(X.values)</span><br><span class="line">y &#x3D; np.matrix(y.values)</span><br><span class="line">theta &#x3D; np.matrix(np.array([0,0]))</span><br></pre></td></tr></table></figure></p>
<h2 id="def-computeCost"><a href="#def-computeCost" class="headerlink" title="def computeCost()"></a>def computeCost()</h2><p>函数中前面注释的三行是计算单变量的,后面两行是计算多变量的<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def computeCost(X, y, theta):</span><br><span class="line">#</span><br><span class="line">#    prediction &#x3D; X * theta.T </span><br><span class="line">#    sqrErrors &#x3D; np.power(prediction-y,2)</span><br><span class="line">#    return np.sum(sqrErrors) &#x2F; (2*len(X))</span><br><span class="line">    prediction &#x3D; X * theta.T </span><br><span class="line">    return (prediction-y).T * (prediction-y) &#x2F; (2*len(X)) </span><br><span class="line">computeCost(X,y,theta)</span><br></pre></td></tr></table></figure><br>执行函数后,结果是32.072733877455676</p>
<h2 id="def-gradientDescent"><a href="#def-gradientDescent" class="headerlink" title="def gradientDescent"></a>def gradientDescent</h2><p>对于每个样本都需要计算,这里应该是<strong>batch gradient decent</strong><br>$\theta _{j}:= \theta _{j} - \alpha \frac{\partial }{\partial \theta _{j}}J(\theta )$<br>注意每次更新都需要同时更新每个$\theta _{j}$<br>吴恩达老师的课特意强调了向量化,它可以让我们有更少的代码量(减少循环),同时也能利用高级语言的函数库更加快速的计算(如矩阵运算,可能可以实现并行计算等)<br>向量化的更新为<br>$\theta:=\theta-\alpha \cdot \delta$<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def gradientDescent(X, y, theta, alpha, num_iters):</span><br><span class="line"></span><br><span class="line">    J_history &#x3D; np.zeros(num_iters)</span><br><span class="line">    </span><br><span class="line">    for i in range(num_iters):</span><br><span class="line">        prediction &#x3D; X * theta.T </span><br><span class="line">        delta &#x3D; X.T * (prediction-y) &#x2F; len(X)</span><br><span class="line">        theta &#x3D; theta - alpha * delta.T </span><br><span class="line">        # print(theta)</span><br><span class="line">        J_history[i] &#x3D; computeCost(X, y, theta)</span><br><span class="line">        </span><br><span class="line">    return theta, J_history</span><br></pre></td></tr></table></figure><br>调用函数执行:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">theta, J &#x3D; gradientDescent(X, y, theta, alpha, num_iters)</span><br></pre></td></tr></table></figure><br>theta = matrix([[-3.63029144,  1.16636235]])</p>
<h2 id="画出拟合的回归方程以及代价函数的变化"><a href="#画出拟合的回归方程以及代价函数的变化" class="headerlink" title="画出拟合的回归方程以及代价函数的变化"></a>画出拟合的回归方程以及代价函数的变化</h2><p>前面我们虽然用来多变量的计算代价函数和梯度下降,但是当我们使用单变量时,我们还可以画出回归线性模型,看看他的拟合效果.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">X &#x3D; np.linspace(data.Population.min(), data.Population.max(), 100)</span><br><span class="line">y &#x3D; theta[0,0] + (theta[0,1] * X)</span><br><span class="line">fig, ax &#x3D; plt.subplots(figsize&#x3D;(12,8))</span><br><span class="line">ax.plot(X, y, &#39;r&#39;, label&#x3D;&#39;Prediction&#39;)</span><br><span class="line">ax.scatter(data.Population, data.Profit, label&#x3D;&#39;Traning Data&#39;)</span><br><span class="line">ax.legend(loc&#x3D;2)</span><br><span class="line">ax.set_xlabel(&#39;Population&#39;)</span><br><span class="line">ax.set_ylabel(&#39;Profit&#39;)</span><br><span class="line">ax.set_title(&#39;Predicted Profit vs. Population Size&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>上述代码运行完,图像是这样的:<br> <img src="/images/onefit.png" width="50%" height="50%"></p>
<p>如何确定梯度下降已经正确的执行了?<br>**在每个迭代之后,$J(\theta)$应该在减小</p>
<ul>
<li>我们可以画出图像来看$J(\theta)$是否收敛</li>
<li>我们还可以设置当$J(\theta)$在某个迭代计算中小于某个值来认为收敛,不过这个值难以确定,因为不同模型的$J$最小值可能是不一样的</li>
</ul>
<p>下面我们画出代价函数的变化<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fig, ax &#x3D; plt.subplots(figsize&#x3D;(12,8))</span><br><span class="line">ax.plot(np.arange(num_iters), J, &#39;r&#39;)</span><br><span class="line">ax.set_xlabel(&#39;Iterations&#39;)</span><br><span class="line">ax.set_ylabel(&#39;Cost&#39;)</span><br><span class="line">ax.set_title(&#39;Error vs. Training Epoch&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br>图像是这样的:<br> <img src="/images/cost.png" width="50%" height="50%"></p>
<p>到这里,已经可以完成coursa上面的作业了<br>不过这里有个小细节需要注意:<br>$\alpha$(学习率)对梯度下降的收敛是有影响的,可以去不同的学习率来测试,我尝试过使用$\alpha=0.1$在单变量的环境下测试,是不会收敛的,一开始还以为是代码哪里写的有问题,后面想到学习率对于梯度下降的影响才恍然大悟。</p>
<p>关于$\alpha$(学习率)对梯度下降的影响:</p>
<ul>
<li>$\alpha$太小: 慢收敛</li>
<li><p>$\alpha$太大:$J(\theta)$在每次迭代过程中可能不会减小,也可能不会收敛(当然也可能会慢收敛)<br>如果我们的$J(\theta)$与iterations的图像是下面这样的,说明梯度下降没有很好的工作,需要使用更小的$\alpha$<br><img src="/images/alpha.png" width="50%" height="50%"><br>实际计算过程中,我们可以尝试选择下面的$\alpha$来计算:</p>
<p>…0.001,0.003,0.01,0.03,0.1,0.3,1…<br>(每次$\times 3$)</p>
</li>
</ul>
<p>下面是关于 特征缩放、正规方程的内容。</p>
<h2 id="特征缩放-feature-scaling"><a href="#特征缩放-feature-scaling" class="headerlink" title="特征缩放(feature scaling)"></a>特征缩放(feature scaling)</h2><p>前面的过程我们并没有将特征进行缩放,使用特征缩放在我们面对多维特征问题的时候,保证这些特征都具有相近的尺度,这将有助于<strong>梯度下降算法更快地收敛</strong>(如果特征差距过大,变量非常不均匀时,它会无效率地<strong>振荡</strong>到最优质,梯度下降算法需要非常多次的迭代才能收敛)<br> <img src="/images/feature scaling.png" width="50%" height="50%"><br>特征缩放一般是将数据减去均值再除以标准差,当然除以数据的最大与最小值之差也是可以的<br>一般来说特征缩放不需要太精确,只需要每个特征roughly the same,这样梯度下降可以运行的更快一些。<br>对于第二个实验数据的正则化,使用python可以这样完成<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">path &#x3D; &#39;ex1data2.txt&#39;</span><br><span class="line">data2 &#x3D; pd.read_csv(path,header&#x3D;None,names&#x3D;[&#39;Size&#39;,&#39;Bedrooms&#39;,&#39;Price&#39;])</span><br><span class="line"># data2.head()</span><br><span class="line">cols &#x3D; data2.shape[1]</span><br><span class="line">X2 &#x3D; data2.iloc[:,0:cols-1]</span><br><span class="line">X2 &#x3D; (X2-X2.mean()) &#x2F; X2.std()</span><br><span class="line">X2.insert(0, &#39;Ones&#39;, 1)</span><br><span class="line">y2 &#x3D; data2.iloc[:,-1]</span><br></pre></td></tr></table></figure></p>
<h2 id="正规方程-Normal-equation"><a href="#正规方程-Normal-equation" class="headerlink" title="正规方程(Normal equation)"></a>正规方程(Normal equation)</h2><p>$\theta = (X^{T}X)^{-1}X^{T} \vec{y}$<br>直接使用这个公式就可以计算出让$J(\theta)$取值最小的$\theta$<br>注意<strong>使用正规方程计算不需要对X进行feature scaling</strong><br>由上面的式子我们可以看出,正规方程主要工作量是在于计算:<br>$(X^{T}X)^{-1}$,计算机计算逆矩阵大约是$O(n^{3})$<br>一般情况下我们是不会出现$(X^{T}X)^{-1}$不存在(逆矩阵不存在)的情况<br>如果出现不可逆的情况,可以考虑:</p>
<ul>
<li><strong>Redundant features</strong>:有冗余特征,他们是线性相关的,这时候可以考虑去除冗余特征</li>
<li><strong>too many features</strong>:假设我们有m个样本,n个特征。如果$m&lt;n$,可能情况是m个样本不够来拟合n个特征的$\theta$,所以可以考虑减少特征数量,还有一种办法是<strong>regularization</strong>,它可以让我们在很少的样本时拟合很多的特征(这个方法吴恩达老师只是简单的介绍,具体应该会在后面的课程中讲述)<br>第三周课程更新:<br>(通过添加正则化项,我们还解决了存在不可逆矩阵的问题)</li>
</ul>
<p>下面是具体的代码实现<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def normalEqn(X, y):</span><br><span class="line">    theta &#x3D; np.linalg.inv(X.T@X)@X.T@y#X.T@X等价于X.T.dot(X)</span><br><span class="line">    return theta</span><br></pre></td></tr></table></figure><br>这里也需要注意一下,<strong>使用正规方程求出来的$\theta_{1}$和我们用梯度下降法求出来的$\theta_{2}$可能是不一样的</strong><br>可以从等高线图来理解,或者老师课上的可视化代价函数,可能有多个$\theta$使得代价函数的最小的。<br>关于正规方程的推导过程我将会在另外的博客中进行说明,因为里面同样涉及到矩阵求导,还有PCA的优化求导,以及我的PR课上老师要求的一个求导小练习一起做个总结。<br>最后是梯度下降与正规方程的比较</p>
<h2 id="梯度下降-VS-正规方程"><a href="#梯度下降-VS-正规方程" class="headerlink" title="梯度下降 VS 正规方程"></a>梯度下降 VS 正规方程</h2><div class="table-container">
<table>
<thead>
<tr>
<th>Gradient Descent</th>
<th>Normal equation</th>
</tr>
</thead>
<tbody>
<tr>
<td>需要选择$\alpha$</td>
<td>不需要选择$\alpha$</td>
</tr>
<tr>
<td>需要多次迭代计算</td>
<td>不需要迭代计算</td>
</tr>
<tr>
<td>当n(特征数量)很大时依旧工作的很好</td>
<td>需要计算$(X^{T}X)^{-1}$逆矩阵的计算需要$O(n^{3})$,通常来说当n小于10000 时还是可以接受的</td>
</tr>
<tr>
<td>适用于各种模型的优化求解</td>
<td>只适用于线性模型，不适合逻辑回归模型等其他更加复杂模型</td>
</tr>
</tbody>
</table>
</div>
<p>bty,在batch gradient decent中复杂度是$O(m \times n) \approx O(n^{2})$</p>
]]></content>
      <categories>
        <category>NG-ML</category>
      </categories>
      <tags>
        <tag>回归</tag>
      </tags>
  </entry>
  <entry>
    <title>NG机器学习EX2(逻辑回归1)</title>
    <url>/2020/11/03/NG%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0EX2(%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%921)/</url>
    <content><![CDATA[<p>本篇博客的主要内容有：</p>
<ol>
<li>逻辑回归理论知识</li>
<li>python代码实现EX2的第一个练习</li>
</ol>
<hr>
<a id="more"></a>
<p>这篇博客的主要来源是吴恩达老师的机器学习第三周的学习课程,还有一些内容参考自<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">黄海广博士的笔记GitHub</a>,先根据作业完成了matlab代码,并且学习将它用python来实现(博客中无matlab实现)。在学习的过程中我结合吴恩达老师的上课内容和matlab代码修改了一部分代码(个人认为可以优化的地方)。</p>
<h2 id="逻辑回归总结"><a href="#逻辑回归总结" class="headerlink" title="逻辑回归总结"></a>逻辑回归总结</h2><h3 id="Logistic-Regression-for-Classification"><a href="#Logistic-Regression-for-Classification" class="headerlink" title="Logistic Regression for Classification"></a>Logistic Regression for Classification</h3><p>线性回归不适合用来解决分类问题,因为分类问题<strong>不是一个线性函数</strong>,加入新的样本,就可能导致线性回归的方程改变了,使得我们没有办法找到正确的划分点,因此没有办法分类。<br>线性回归问题其$h_{\theta}(x)$的值可能是大于1或者小于0的<br>而对于分类问题,<strong>我们的输出</strong>只有0或者1(二分类),因此$h_{\theta}(x)$太大或者太小都不行,我们可以用逻辑函数（logistic function,也可以叫做sigmoid函数)来使$0\leq h_{\theta}(x) \leq 1$<br><strong>不要被回归所误导,逻辑回归解决的是分类问题</strong></p>
<h3 id="Hypothesis-Representation"><a href="#Hypothesis-Representation" class="headerlink" title="Hypothesis Representation"></a>Hypothesis Representation</h3><p>逻辑回归模型的 $h_\theta(X)=g(\theta^{T}X)$</p>
<p>$X$ 代表特征向量<br>$g$ 代表逻辑函数（<strong>logistic function</strong>)是一个常用的逻辑函数为<strong>S</strong>形函数（<strong>Sigmoid function</strong>），公式为： $g(z)= \frac{1}{1 + e^{-z}}$<br>这里有个概念非常重要,那就是:<br>$h_\theta(X)$ = estimated probability that y=1 on input x<br>也就是说,经过sigmoid函数作用后的$h_\theta(X)$,输出的是y=1的概率!<br>也就是说:<br>$h_\theta(X) = P(y=1 | x;\theta)$</p>
<h3 id="Decision-boundary"><a href="#Decision-boundary" class="headerlink" title="Decision boundary"></a>Decision boundary</h3><p>我们假设,<br>$h_\theta(X) \geq 0.5$时 y=1 也就是$\theta^{T}X \geq 0$<br>$h_\theta(X) &lt; 0.5$时 y=0 也就是$\theta^{T}X &lt; 0$<br>这里的等号不需要纠结<br>也就是说$\theta^{T}X = 0$就是我们的决策边界<br>对于简单的线性可分的边界,我们可以直接画出来一条直线<br>对于复杂的比如不能非线性可分的,我们可以通过添加多项式项,得到更加复杂的决策边界</p>
<h3 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h3><p>如果我们和线性回归一样,定义代价函数为:<br>    $J\left( {\theta_{0}},{\theta_{1}}…{\theta_{n}} \right)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta }(x^{(i)})-y^{(i)})^{2}$<br>那么将我们的$h_\theta(X)$ (非线性函数 non-linear function)代入,将会得到一个非凸函数,有许多的局部最小值,我们使用梯度下降就不能保证收敛到全局最小值,如下图所示:<br><img src="/images/4 NG EX2/convex.jpg" width="50%" height="50%"> </p>
<p>因此我们的代价函数定义为:<br>$J_{\theta} = \frac{1}{m}\sum_{i=1}^{m}cost(h_{\theta}(x^{(i)}),y)$<br>其中<br>$cost(h_{\theta}(x),y)=\left\{\begin{matrix}<br>-log(h_{\theta}(x)) .. if..y=1<br>\\<br>-log(1-h_{\theta}(x))  .. if ..y=0<br>\end{matrix}\right.$</p>
<p>下图画出了cost的图像<br><img src="/images/4 NG EX2/cost.jpg" width="50%" height="50%"> </p>
<p>下面我们讨论这个公式的意义<br><strong>y = 1</strong>时</p>
<ul>
<li>cost = 0 if y = 1,$h_{\theta}(x)=1$<br>  但是 当$h_{\theta}(x)=1 \rightarrow 0$时,$cost \rightarrow  \infty $</li>
<li>也就是: if $h_{\theta}(x)=0$(预测$P(y=1|x;\theta) =0$,但是实际上样本分类为y=1,这样我们将进行惩罚,penalize learning algorithm by a very large cost</li>
</ul>
<p>同理 y=0(样本正确分类是0)时,当$h_{\theta}(x)=1 \rightarrow 0$时,预测$P(y=1|x;\theta) =1$,$cost \rightarrow  \infty $(设置无穷大的cost)</p>
<p>总结下,就是下面的意思(可以结合上面的图像来看):<br>$cost=0, if h_{\theta}(x) = y$<br>$cost \rightarrow \infty, if y=0 and h_{\theta}(x) \rightarrow 1$<br>$cost \rightarrow \infty, if y=1 and h_{\theta}(x) \rightarrow 0$<br>这样我们通过减少cost,就能找到目标$\theta$啦<br>同时<strong>我们这里的cost能够保证$J(\theta)$是凸函数</strong>(NG老师貌似讲了一个极大似然估计然后说是凸的,这里我是一只鸵鸟跳过)</p>
<p>下面是cost的简单写法:<br>$cost = -y^{i}log(h_{\theta}(X^{(i)}))-(1-y^{i})log(1-h_{\theta}(x^{(i)}))$<br>可以验证y=1或者0代入时就是刚刚前面的cost<br>那么我们有<br>$J(\theta) = \frac{1}{m}\sum_{i=1}^{m}[-y^{i}log(h_{\theta}(X^{(i)}))-(1-y^{i})log(1-h_{\theta}(x^{(i)}))]$<br>其梯度定义为:<br>$\frac{\partial J(\theta)}{\partial \theta_{j}} = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}$<br>求偏导数的求导过程如下,比较麻烦我直接贴图,不用markdown公式了,不过推导起来难度不大。<br><img src="/images/4 NG EX2/grad.png" width="50%" height="50%"> </p>
<p>算出来梯度后,我们会发现,逻辑回归和线性回归的梯度是一样的形式,但是由于$h_{\theta}(x)$是不一样的,所以梯度是不一样的(<del>这可能就是为啥叫逻辑<strong>回归</strong>的原因?</del>)<br>接下来我们就可以使用梯度下降法来优化$\theta$了。</p>
<h3 id="梯度下降法的向量化写法"><a href="#梯度下降法的向量化写法" class="headerlink" title="梯度下降法的向量化写法"></a>梯度下降法的向量化写法</h3><p>在实际的代码实现中,我们一般把每个特征写成行向量(n个维度),m个样本组成了$X_{m \times n}$矩阵,我们会将$\theta$写成列向量$\theta_{n \times 1}$<br>这样有<br>$h = g(X\theta)$</p>
<p>$J_{\theta} = \frac{1}{m}\sum_{i=1}^{m}[-y^{i}log(h_{\theta}(X^{(i)}))-(1-y^{i})log(1-h_{\theta}(x^{(i)}))]$</p>
<p>Grandient Descent:<br>repeat{<br>    $\theta_{j} := \theta_{j} - \alpha \cdot \frac{\partial J(\theta)}{\partial \theta_{j}} $<br>}</p>
<p>其中<br>$\frac{\partial J(\theta)}{\partial \theta_{j}} = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)})$</p>
<p><strong>更新$\theta$的向量化表示:</strong><br>$\theta := \theta - \frac{\alpha}{m} \cdot X^{T} \cdot (g(X\theta)-\vec{y})$</p>
<h3 id="使用matlab或者python的优化算法来优化-theta"><a href="#使用matlab或者python的优化算法来优化-theta" class="headerlink" title="使用matlab或者python的优化算法来优化$\theta$"></a>使用matlab或者python的优化算法来优化$\theta$</h3><p>梯度下降法是一种常用的优化算法,但是还存在其他的优化算法如:</p>
<ul>
<li><del>Gradient descent</del></li>
<li>Conjugate gradien</li>
<li>BFGS</li>
<li>L-BFGS</li>
</ul>
<p>相比较梯度下降,这些算法的有点在于:</p>
<ul>
<li>不需要我们人工的选择$\alpha$</li>
<li>通常会比梯度下降计算的快</li>
</ul>
<p>但是这些Advanced optimization也更加复杂,算法的性能与实现的好坏有关系<br>这些算法理解起来难度比较大,但是这些复杂的理论就交给数学家们去研究吧,我们当一只鸵鸟,会用即可,不需要知道其内部实现</p>
<h3 id="Multi-class-classification-One-Vs-All"><a href="#Multi-class-classification-One-Vs-All" class="headerlink" title="Multi-class classification One-Vs-All"></a>Multi-class classification One-Vs-All</h3><p>对于多分类问题,我们可以每次将一类样本当作一类,其他所有样本当作另外一类,c类样本的话总共需要c个分类器,当我们用来预测时,只需要将新的样本输入,然后选择c个分类器中概率最大的那个</p>
<p>训练逻辑回归分类器$h_{\theta}^{(i)}(x)$<br>对于每一类$i$ 来预测$y=i$的概率<br>对新的$x$,我们有$class = max_{i}(h_{\theta}^{(i)}(x))$</p>
<p>下面是具体的代码实现</p>
<h2 id="python代码实现"><a href="#python代码实现" class="headerlink" title="python代码实现"></a>python代码实现</h2><h3 id="读入数据并可视化"><a href="#读入数据并可视化" class="headerlink" title="读入数据并可视化"></a>读入数据并可视化</h3><p>学生两次考试的成绩和最后是否被录取的结果来训练回归模型(分类),再用它来做预测。</p>
<ol>
<li><p>先读入数据,并且输出数据前几行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">path &#x3D; &#39;ex2data1.txt&#39;</span><br><span class="line">data &#x3D; pd.read_csv(path,header&#x3D;None,names&#x3D;[&#39;Exam 1&#39;,&#39;Exam 2&#39;,&#39;Admitted&#39;])</span><br><span class="line">print(data.head())</span><br></pre></td></tr></table></figure>
</li>
<li><p>找到分类为1和分类为0的,并用不同颜色将他们画出来</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def plotData(positive,negative):</span><br><span class="line">    fg,ax &#x3D; plt.subplots(figsize &#x3D; (12,8))</span><br><span class="line">    ax.scatter(positive[&#39;Exam 1&#39;],positive[&#39;Exam 2&#39;],s&#x3D;50,c&#x3D;&#39;y&#39;,marker&#x3D;&#39;o&#39;,label&#x3D;&#39;Admitted&#39;)</span><br><span class="line">    ax.scatter(negative[&#39;Exam 1&#39;],negative[&#39;Exam 2&#39;],s&#x3D;50,c&#x3D;&#39;r&#39;,marker&#x3D;&#39;o&#39;,label&#x3D;&#39;Not Admitted&#39;)</span><br><span class="line"></span><br><span class="line">    ax.legend()</span><br><span class="line">    ax.set_xlabel(&#39;Exam 1 Score&#39;)</span><br><span class="line">    ax.set_ylabel(&#39;Exam 2 Score&#39;)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">positive &#x3D; data[data[&#39;Admitted&#39;].isin([1])]</span><br><span class="line">negative &#x3D; data[data[&#39;Admitted&#39;].isin([0])]</span><br><span class="line">plotData(positive,negative)</span><br></pre></td></tr></table></figure>
<p><img src="/images/4 NG EX2/data1.png" width="50%" height="50%"> </p>
<p>仔细观察图像,这两类样本之间存在清晰的决策边界,因此我们可以训练逻辑回归模型。</p>
<h3 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h3><p>$h_\theta(X)=g(\theta^{T}X)$<br>$g(z)= \frac{1}{1 + e^{-z}}$<br>其代码实现为:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def sigmoid(z):</span><br><span class="line">    return 1&#x2F;(1+np.exp(-z))</span><br></pre></td></tr></table></figure>
<p>我们可以画出sigmoid函数的图像来看看我们的函数是否写的正确:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def checkSigmoid():</span><br><span class="line">    fig,ax &#x3D; plt.subplots(figsize&#x3D;(12,8))</span><br><span class="line">    nums &#x3D; np.arange(-10,10,step&#x3D;0.1)</span><br><span class="line">    ax.plot(nums,sigmoid(nums),&#39;r&#39;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/4 NG EX2/sigmoid.png" width="50%" height="50%"><br>从图像来看,我们的sigmoid函数写的是没有问题的</p>
</li>
</ol>
<h3 id="计算代价函数和梯度"><a href="#计算代价函数和梯度" class="headerlink" title="计算代价函数和梯度"></a>计算代价函数和梯度</h3><p>$J_{\theta} = \frac{1}{m}\sum_{i=1}^{m}[-y^{i}log(h_{\theta}(X^{(i)}))-(1-y^{i})log(1-h_{\theta}(x^{(i)}))]$</p>
<p>$\frac{\partial J(\theta)}{\partial \theta_{j}} = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)})$</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def costFunction(theta,X,y):</span><br><span class="line">    h &#x3D; sigmoid(X.dot(theta.T))</span><br><span class="line">    J &#x3D;(- y.T .dot(np.log(h))-(1-y).T.dot(np.log(1-h) ))&#x2F;len(X)</span><br><span class="line">    return J</span><br><span class="line"></span><br><span class="line">def gradFunction(theta,X,y):</span><br><span class="line">    grad &#x3D; np.zeros(len(theta))</span><br><span class="line">    theta &#x3D; theta.reshape(1,len(theta))</span><br><span class="line">    h &#x3D; sigmoid(X.dot(theta.T))</span><br><span class="line">    grad &#x3D; X.T.dot ((h-y)) &#x2F; len(X)</span><br><span class="line">    return grad</span><br></pre></td></tr></table></figure>
<p>定义完这两个函数,我们设置数据,进行验证<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data.insert(0, &#39;Ones&#39;, 1)</span><br><span class="line">cols &#x3D; data.shape[1]</span><br><span class="line">X &#x3D; data.iloc[:,0:cols-1]</span><br><span class="line">y &#x3D; data.iloc[:,cols-1:cols]</span><br><span class="line">X &#x3D; np.array(X.values)</span><br><span class="line">y &#x3D; np.array(y.values)</span><br><span class="line">theta &#x3D; np.zeros(3)</span><br></pre></td></tr></table></figure><br>这里是直接使用<strong>numpy的array数组</strong>,因为在后面使用python的优化函数时候,遇到了维数问题报错((100, 1)和(100,),我建议别转成矩阵,直接用array来计算</p>
<blockquote>
<p>Basically you are running up against the fact that np.matrix is not fully supported by many functions in numpy and scipy that expect an np.ndarray. I highly recommend you switch from using np.matrix to np.ndarray - use of np.matrix is officially discouraged, and will probably be deprecated in the not-too-distant future.<a href="https://stackoverrun.com/cn/q/9331088" target="_blank" rel="noopener">可以看这个解释</a></p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(costFunction(theta,X,y),gradFunction(theta,X,y))</span><br></pre></td></tr></table></figure>
<p>调用函数后,出现下面的结果,就说明我们计算代价函数和梯度的代码没有问题了(第一个是$J_{\theta}$,第二个是$\frac{\partial J(\theta)}{\partial \theta_{j}}$<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[0.69314718] </span><br><span class="line">[[ -0.1       ]</span><br><span class="line"> [-12.00921659]</span><br><span class="line"> [-11.26284221]]</span><br></pre></td></tr></table></figure></p>
<h3 id="优化-theta"><a href="#优化-theta" class="headerlink" title="优化$\theta$"></a>优化$\theta$</h3><p>在NG老师的练习中,是使用一个称为“fminunc”的Octave函数是用来优化函数来计算成本和梯度参数。由于我们使用Python,我们可以用SciPy的“optimize”函数来优化。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import scipy.optimize as opt</span><br><span class="line">result &#x3D; opt.fmin_tnc(func&#x3D;costFunction,x0&#x3D;theta,fprime&#x3D;gradFunction,args&#x3D;(X,y))</span><br><span class="line">print(result)</span><br><span class="line">print(costFunction(result[0],X,y))</span><br></pre></td></tr></table></figure>
<p>上面的opt函数有一些注意点:</p>
<ul>
<li>costFunction和gradFunction这两个函数的定义上注意先让theta在前面,再定义参数X,y,下面的x0是传入我们初始的$\theta$,我们的目标就是要找到最优的$\theta$,让$J_{\theta}$最小</li>
<li>最后的args就是我们前面应该要传入cost和grad函数的参数</li>
<li>注意维度问题(前面已经提过,我建议使用array,不一定非要用matrix)</li>
<li>返回值中第一个就是我们的目标$\theta$</li>
</ul>
<p>上面的代码执行后,应该是下面的结果:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(array([-25.1613186 ,   0.20623159,   0.20147149]), 36, 0)</span><br><span class="line">[0.2034977]</span><br></pre></td></tr></table></figure><br>其中上面第二行是使用优化后的$\theta$计算的$J_{\theta}$</p>
<p>接下来我们就可以使用$\theta$画出决策边界了:<br>决策边界是一条直线,我们只需要输入两个端点,即可画出直线<br>这里X坐标取得是X数据中x1的最大值或者最小值+-2<br>Y坐标的话是根据<br>$h_{\theta}(X) = \theta_{0} + \theta{1}x_{1} + \theta{2}x_{2}$来推得:<br>$x_{2} = \frac{ -\theta_{0}-\theta{1}x_{1}}{\theta{2}}$<br>$x_{2}$就是我们点的Y坐标<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">theta_min &#x3D; result[0]</span><br><span class="line">pointX &#x3D; np.array([np.min(X[:,1])-2,np.max(X[:,2])+2]) </span><br><span class="line">pointY &#x3D; (- theta_min[0] - theta_min[1] * pointX)&#x2F;theta_min[2]</span><br><span class="line">fig,ax &#x3D; plt.subplots(figsize&#x3D;(12,8))</span><br><span class="line">ax.plot(pointX,pointY,linewidth&#x3D;5)</span><br><span class="line">ax.scatter(positive[&#39;Exam 1&#39;],positive[&#39;Exam 2&#39;],s&#x3D;50,c&#x3D;&#39;y&#39;,marker&#x3D;&#39;o&#39;,label&#x3D;&#39;Admitted&#39;)</span><br><span class="line">ax.scatter(negative[&#39;Exam 1&#39;],negative[&#39;Exam 2&#39;],s&#x3D;50,c&#x3D;&#39;r&#39;,marker&#x3D;&#39;o&#39;,label&#x3D;&#39;Not Admitted&#39;)</span><br><span class="line">ax.legend()</span><br><span class="line">ax.set_xlabel(&#39;Exam 1 Score&#39;)</span><br><span class="line">ax.set_ylabel(&#39;Exam 2 Score&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br>根据上面的代码,我们可以画出下面的图像:<br> <img src="/images/4 NG EX2/boundary.png" width="50%" height="50%"> </p>
<h3 id="predict"><a href="#predict" class="headerlink" title="predict"></a>predict</h3><p> 接下来我们就可以定义预测函数用于新的数据来测试了,下面是使用原有的数据集来计算accuracy<br> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> def predict(theta,X):</span><br><span class="line">    probality &#x3D; sigmoid(X.dot(theta.T))</span><br><span class="line">    return [1 if x &gt;&#x3D; 0.5 else 0 for x in probality]</span><br><span class="line"></span><br><span class="line">theta_min &#x3D; result[0]</span><br><span class="line">predictions &#x3D; predict(theta_min,X)</span><br><span class="line">correct &#x3D; [1 if (a &#x3D;&#x3D; 1 and b &#x3D;&#x3D; 1) or (a &#x3D;&#x3D; 0 and  b &#x3D;&#x3D; 0) else 0 for (a,b) in zip(predictions,y)]</span><br><span class="line"></span><br><span class="line">accuracy &#x3D; sum(map(int,correct)) * 100 &#x2F;len(correct)</span><br><span class="line">print(&#39;accuracy &#x3D; &#123;0:.2f&#125;%&#39;.format(accuracy))</span><br></pre></td></tr></table></figure><br>上述代码运行后,得到的结果为:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">accuracy &#x3D; 89.00%</span><br></pre></td></tr></table></figure></p>
<h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np </span><br><span class="line">import pandas as pd </span><br><span class="line">import matplotlib.pyplot as plt </span><br><span class="line">import scipy.optimize as opt</span><br><span class="line">import pdb</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def sigmoid(z):</span><br><span class="line">    return 1&#x2F;(1+np.exp(-z))</span><br><span class="line"></span><br><span class="line">def plotData(positive,negative):</span><br><span class="line">    fg,ax &#x3D; plt.subplots(figsize &#x3D; (12,8))</span><br><span class="line">    ax.scatter(positive[&#39;Exam 1&#39;],positive[&#39;Exam 2&#39;],s&#x3D;50,c&#x3D;&#39;y&#39;,marker&#x3D;&#39;o&#39;,label&#x3D;&#39;Admitted&#39;)</span><br><span class="line">    ax.scatter(negative[&#39;Exam 1&#39;],negative[&#39;Exam 2&#39;],s&#x3D;50,c&#x3D;&#39;r&#39;,marker&#x3D;&#39;o&#39;,label&#x3D;&#39;Not Admitted&#39;)</span><br><span class="line"></span><br><span class="line">    ax.legend()</span><br><span class="line">    ax.set_xlabel(&#39;Exam 1 Score&#39;)</span><br><span class="line">    ax.set_ylabel(&#39;Exam 2 Score&#39;)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def checkSigmoid():</span><br><span class="line">    fig,ax &#x3D; plt.subplots(figsize&#x3D;(12,8))</span><br><span class="line">    nums &#x3D; np.arange(-10,10,step&#x3D;0.1)</span><br><span class="line">    ax.plot(nums,sigmoid(nums),&#39;r&#39;)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">def costFunction(theta,X,y):</span><br><span class="line"></span><br><span class="line">    h &#x3D; sigmoid(X.dot(theta.T))</span><br><span class="line">    J &#x3D;(- y.T .dot(np.log(h))-(1-y).T.dot(np.log(1-h) ))&#x2F;len(X)</span><br><span class="line"></span><br><span class="line">    return J</span><br><span class="line"></span><br><span class="line">def gradFunction(theta,X,y):</span><br><span class="line">    grad &#x3D; np.zeros(len(theta))</span><br><span class="line">    theta &#x3D; theta.reshape(1,len(theta))</span><br><span class="line">    h &#x3D; sigmoid(X.dot(theta.T))</span><br><span class="line">    grad &#x3D; X.T.dot ((h-y)) &#x2F; len(X)</span><br><span class="line">    return grad</span><br><span class="line"></span><br><span class="line">def predict(theta,X):</span><br><span class="line">    probality &#x3D; sigmoid(X.dot(theta.T))</span><br><span class="line">    return [1 if x &gt;&#x3D; 0.5 else 0 for x in probality]</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line"></span><br><span class="line">    path &#x3D; &#39;ex2data1.txt&#39;</span><br><span class="line">    data &#x3D; pd.read_csv(path,header&#x3D;None,names&#x3D;[&#39;Exam 1&#39;,&#39;Exam 2&#39;,&#39;Admitted&#39;])</span><br><span class="line">    print(data.head())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    positive &#x3D; data[data[&#39;Admitted&#39;].isin([1])]</span><br><span class="line">    negative &#x3D; data[data[&#39;Admitted&#39;].isin([0])]</span><br><span class="line"></span><br><span class="line">    # plotData(positive,negative)</span><br><span class="line">    # checkSigmoid()</span><br><span class="line">   </span><br><span class="line">    data.insert(0, &#39;Ones&#39;, 1)</span><br><span class="line">    cols &#x3D; data.shape[1]</span><br><span class="line">    X &#x3D; data.iloc[:,0:cols-1]</span><br><span class="line">    y &#x3D; data.iloc[:,cols-1:cols]</span><br><span class="line">    X &#x3D; np.array(X.values)</span><br><span class="line">    y &#x3D; np.array(y.values)</span><br><span class="line">    theta &#x3D; np.zeros(3)</span><br><span class="line">    # print(costFunction(theta,X,y),gradFunction(theta,X,y))</span><br><span class="line">    result &#x3D; opt.fmin_tnc(func&#x3D;costFunction,x0&#x3D;theta,fprime&#x3D;gradFunction,args&#x3D;(X,y))</span><br><span class="line"></span><br><span class="line">    # print(costFunction(result[0],X,y))</span><br><span class="line"></span><br><span class="line">    theta_min &#x3D; result[0]</span><br><span class="line">    </span><br><span class="line">    predictions &#x3D; predict(theta_min,X)</span><br><span class="line"></span><br><span class="line">    correct &#x3D; [1 if (a &#x3D;&#x3D; 1 and b &#x3D;&#x3D; 1) or (a &#x3D;&#x3D; 0 and  b &#x3D;&#x3D; 0) else 0 for (a,b) in zip(predictions,y)]</span><br><span class="line">    </span><br><span class="line">    accuracy &#x3D; sum(map(int,correct)) * 100 &#x2F;len(correct)</span><br><span class="line">    print(&#39;accuracy &#x3D; &#123;0:.2f&#125;%&#39;.format(accuracy)) </span><br><span class="line"></span><br><span class="line">    #下面是画决策边界</span><br><span class="line">    # print([np.max(X[:,1],axis&#x3D;0),np.max(X[:,2],axis&#x3D;0))]</span><br><span class="line">    pointX &#x3D; np.array([np.min(X[:,1])-2,np.max(X[:,2])+2]) </span><br><span class="line">    print(pointX)</span><br><span class="line">    pointY &#x3D; (- theta_min[0] - theta_min[1] * pointX)&#x2F;theta_min[2]</span><br><span class="line">    print(pointY)</span><br><span class="line"></span><br><span class="line">    fig,ax &#x3D; plt.subplots(figsize&#x3D;(12,8))</span><br><span class="line">    ax.plot(pointX,pointY,linewidth&#x3D;5)</span><br><span class="line">    ax.scatter(positive[&#39;Exam 1&#39;],positive[&#39;Exam 2&#39;],s&#x3D;50,c&#x3D;&#39;y&#39;,marker&#x3D;&#39;o&#39;,label&#x3D;&#39;Admitted&#39;)</span><br><span class="line">    ax.scatter(negative[&#39;Exam 1&#39;],negative[&#39;Exam 2&#39;],s&#x3D;50,c&#x3D;&#39;r&#39;,marker&#x3D;&#39;o&#39;,label&#x3D;&#39;Not Admitted&#39;)</span><br><span class="line">    ax.legend()</span><br><span class="line">    ax.set_xlabel(&#39;Exam 1 Score&#39;)</span><br><span class="line">    ax.set_ylabel(&#39;Exam 2 Score&#39;)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">黄海广博士的笔记GitHub</a></li>
</ol>
]]></content>
      <categories>
        <category>NG-ML</category>
      </categories>
      <tags>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title>PCA学习笔记(含推导过程)</title>
    <url>/2020/10/18/PCA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E5%90%AB%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B)/</url>
    <content><![CDATA[<p>本篇博客的主要内容有：</p>
<ol>
<li>降维</li>
<li>PCA的数学基础</li>
<li>PCA的两种思想推导<ul>
<li>基于最大投影方差</li>
<li>基于最小重构距离</li>
</ul>
</li>
<li>PCA的计算过程</li>
</ol>
<hr>
<a id="more"></a>
<h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><p>降维属于<strong>无监督学习</strong>，可以用来解决训练中<strong>过拟合</strong>和<strong>维数灾难</strong>问题。</p>
<ol>
<li><p><strong>维数灾难</strong> 导致数据稀疏性/过拟合<br>一个直观的理解是：假设我们有十个样本<br>特征维数为1D时，当特征空间是长度为5的线段时，样本密度是10/5=2。<br>特征维数为2D时，特征空间大小为$5^{2}$，样本密度为10/25=0.4。<br>特征维数为3D时，特征空间大小为$5^{3}$，样本密度为10/125=0.08<br>如果我们继续增加特征，特征空间的维数就会不断增长，变得越来越稀疏。由于这种稀疏性，当特征的数量无穷大时，训练样本位于最佳超平面错误一侧的可能性无穷小，因此找到可分离超平面就变得容易得多。事实上，增加特征数量使得高维空间线性可分，相当于在低维空间内训练一个复杂的非线性分类器。不过，这个非线性分类器学习了训练数据中的specific instances and exceptions，这就直接导致导致了<strong>过拟合</strong></p>
</li>
<li><p><strong>过拟合(overfitting)</strong> 使用太多特征将会导致过拟合，泛化能力差<br><strong>overfitting</strong>: The classifier starts learning exceptions that are specific to the training data and do not generalize well when new data is encountered.<br>虽然在较低维度下分类器的性能表现的可能不如在高维空间中的好，但是对于未见过的数据却表现了良好的泛化能力，也就是说，使用更少的特征，我们的分类器可以<strong>避免过拟合</strong>，进而<strong>避免维数灾难</strong></p>
</li>
<li><p>维数灾难不同视角一<br>假设每个样本的特征值都是唯一的，并且每个特征的范围都是[0-1]<br>在1D时，如果我们想要将训练数据覆盖特征范围的20%时，我们需要所有样本的20%作为训练数据<br>在2D时，0.45^2=0.2，因此我们需要所有样本的45%作为训练数据<br>在3D时，0.58^3=0.2，因为我们需要所有样本的58%作为训练样本<br>也就是说，随着特征维数的增加，为了覆盖特征值值域的20%，我们需要增加更多的训练样本，当<strong>训练样本数量固定时，增加更多的特征维数，就容易导致过拟合</strong></p>
</li>
<li><p>维数灾难中不同视角二<br>维数灾难导致<strong>训练样本的分布不均</strong> 在中心位置的数据比在其周围的数据分布更加的稀疏，更多的数据将会分布在超立方体的角落。<br>可以这样来理解，设我们的立方体的边长为1。<br>不管维数多少，我们的超立方体的体积都为1<br>超球体的体积计算公式为：<br>$V_{超球体} = K \cdot  r ^{D}$   （r为小于1的数）<br>当D-&gt;无穷大时，V-&gt;0，这部分说明了分类问题中的“维数灾难”：<strong>在高维特征空间中，大多数的训练样本位于超立方体的角落。</strong></p>
</li>
<li><p>维数灾难中不同视角三<br>想象我们有一个圆环，其外环半径为1，内环半径为$1-\varepsilon$<br>可以求得：<br>$V_{外} = K \cdot 1 ^{D} = K$<br>$V_{环} =V_{外} - V_{内} =  K - K \cdot (1-\varepsilon )^{D}$<br>从而有：<br>$\frac{V_{外}}{V_{内}}  =  1 - (1-\varepsilon ) ^{D}$<br>$\lim_{D \to \infty }\frac{V_{外}}{V_{内}} = 1$<br><strong>随着维数的增加，我们在三维空间中的几何直觉会在考虑高维空间中不起作用。</strong> 一个球体的大部分体积都聚集在表面附近的薄球壳上面。</p>
</li>
<li><p>降维的方法分类<br>对于数据降维的方法有</p>
<ul>
<li>直接降维(特征选择)</li>
<li>线性降维(PCA等)</li>
<li>非线性降维(流行学习等)<br>下面将开始介绍PCA降维的思想。</li>
</ul>
</li>
</ol>
<h2 id="PCA的基础概念"><a href="#PCA的基础概念" class="headerlink" title="PCA的基础概念"></a>PCA的基础概念</h2><ol>
<li><p>求和向量$1_{n}$<br>对于任何一个向量x，其所有元素之和都等于x与求和向量的内积</p>
</li>
<li><p>中心化矩阵(centering matrix)<br>$H = I_{N} - \frac{1}{N}\cdot 1_{N}\cdot 1_{N}^{T}$<br>对于任意的向量x,$H\cdot x$表示将原数据向量的各个元素减去n个数据的均值的结果,直观上理解就是将数据往中心原点移动,中心化后的数据均值为0。<br>中心化矩阵有两个基本性质：</p>
</li>
</ol>
<ul>
<li>$H^{T} = H$</li>
<li>$H^{n} = H$<br>以上两个将会在推导PCoA中使用到</li>
</ul>
<ol>
<li><p>Data:<br>$X_{N\times P} = \begin{pmatrix}<br>x_{1}&amp; x_{2} &amp; …  &amp; x_{n}<br>\end{pmatrix}^{T}$<br>表示有N个数据，每个数据是P维的</p>
</li>
<li><p>Sample Mean:<br>$\bar{X} = \frac{1}{N} \sum_{i=1}^{N}x_{i}$</p>
</li>
<li><p>Sample Covariance<br>对于一维数据，我们有：<br>$S = \frac{1}{N} \sum_{i=1}^{N}(x_{i}-\bar{x})^{2} = \frac{1}{N}\cdot X^{T}\cdot 1_{N}$<br>对于p维数据，我们有：<br>$S_{p\times p} = \frac{1}{N} \sum_{i=1}^{N}(x_{i}-\bar{x})\cdot (x_{i}-\bar{x})^{T} = \frac{1}{N}\cdot X^{T}\cdot H\cdot X$</p>
</li>
<li><p>计算向量在某个方向上的投影<br>假设有两个向量$\vec{a},\vec{b}$,则$\vec{a}$在$\vec{b}$上面的投影$P_{1} = |\vec{a}| \cdot  \cos \theta$<br>同时我们有$\vec{a}\cdot \vec{b} = \left | \vec{a} \right |\cdot \left | \vec{b} \right |\cdot \cos \theta$<br>因此，当$\left | \vec{b} \right | = 1$时,<br>$\vec{a}\cdot \vec{b} =  \left | \vec{a} \right |\cdot\cos \theta = P_{2}$<br>我们可以看到,当$\left | \vec{b} \right | = 1$,$P_{1} = P_{2}$<br>因此算投影可以直接算向量点积,在数学中我们一般写成下面的形式: $P = a^{T} \cdot b$<br>s.t. $\left | \vec{b} \right | = 1$</p>
</li>
</ol>
<h2 id="PCA的思想与推导过程"><a href="#PCA的思想与推导过程" class="headerlink" title="PCA的思想与推导过程"></a>PCA的思想与推导过程</h2><p>PCA的主要内容可以用一句话来概括，那就是：<strong>一个中心，两个基本点</strong><br><strong>一个中心</strong>：是对原始特征空间的重构，将一组可能是线性相关的数据通过正交变换，变成线性无关的数据(即去除数据的线性相关性)，当重构的空间维数小于原始特征空间维数时，就达到了降维的目的。<br><strong>两个基本点</strong>：</p>
<ul>
<li>最大投影方差思想</li>
<li>最小重构距离思想</li>
</ul>
<h3 id="基于最大投影方差思想的推导过程"><a href="#基于最大投影方差思想的推导过程" class="headerlink" title="基于最大投影方差思想的推导过程"></a>基于最大投影方差思想的推导过程</h3><p>最大投影方差指：<br>将样本投影到新的坐标下，数据能够尽可能的分开(也就是方差尽可能大)，这样就能尽可能的将降低数据维度的同时更好的保留原始数据的特性，更好的区分原始数据。<br>我们先计算将数据降至1维，投影方向$\vec{u_{1}}$</p>
<p>$j = \overrightarrow{(x_{i}-\bar{x})^{T}}\cdot \vec{u_{1}}$<br>s.t. $\vec{|u_{1}|}=1(u_{1}^{T} \cdot u_{1}=1)$<br>实际上投影是一个数:以$u_{1}$为基底时，在$u_{1}$方向上的坐标<br>那么投影方差应该表示为：<br>$J = \frac{1}{N}\sum_{i=1}^{N}\begin{Bmatrix}<br>(x_{i}-\bar{x})^{T}\cdot u_{1}<br>\end{Bmatrix}^{2}$<br>(这里要注意,因为前面已经中心化,均值已经为0了,所以直接把投影做平方累加就好)<br>$= \frac{1}{N}\sum_{i=1}^{N}u_{1}^{T}\cdot (x_{i}-\bar{x})\cdot (x_{i}-\bar{x})^{T}\cdot u_{1}$<br>$=u_{1}^{T}\cdot \bigl(\begin{smallmatrix}<br>\frac{1}{N}\sum_{i=1}^{N}<br>(x_{i}-\bar{x})\cdot (x_{i}-\bar{x})^{T}<br>\end{smallmatrix}\bigr)\cdot u_{1}$<br>$= u_{1}^{T}\cdot S \cdot u_{1}$</p>
<p>到这里我们就得到了J的表示函数，就将问题转为求：<br>$\check{u_{1}} = argmax ( u_{1}^{T}\cdot S \cdot u_{1})$<br>s.t. $u_{1}^{T} \cdot u_{1} = 1$</p>
<p>根据拉格朗日乘子法<br>$\alpha (u_{1},\lambda ) = u_{1}^{T}\cdot S \cdot u_{1})  + \lambda \cdot (1-u_{1}^{T}\cdot u_{1})$<br>有<br>$\frac{\partial \alpha}{\partial u_{1}} = 2\cdot S\cdot u_{1} - \lambda \cdot 2\cdot u_{1}=0$<br>$Su_{1} = \lambda u_{1}$</p>
<p>这里面涉及到矩阵求导，请参考：<br><a href="https://zhuanlan.zhihu.com/p/24709748" target="_blank" rel="noopener">矩阵求导术</a></p>
<p>实际上式子推导到这里，我们可以得出结论：<br>$\lambda$即特征值，$u_{1}$即特征向量<br>也就是说，我们的投影方向应该是S的特征向量的方向<br>再接着我们将上面的式子左乘$u_{1}^{T}$可以得到：<br>$u_{1}^{T}\cdot S \cdot u_{1} = J = \lambda$<br>注意这里$\lambda$刚好就是等于我们的$J$<br>也就是说，如果将数据降至一维，那么我们的$\check{u_{1}} = argmax ( u_{1}^{T}\cdot S \cdot u_{1}) $ 极大值就是$\lambda$<br>即$u_{1}$是S最大特征值对应的特征向量时，投影方差取到极大值，我们称$u_{1}$为<strong>第一主成分</strong><br>那么接下来我们就先获得方差最大的1维，生成该维的补空间；，继续在补空间中获得方差最大的1维，继续生成新的补空间，依次循环下去得到q维的空间，我们就可以降至q维了。<br>S的特征向量，就可以认为是<strong>主成分</strong></p>
<h3 id="基于最小重构距离思想的推导过程"><a href="#基于最小重构距离思想的推导过程" class="headerlink" title="基于最小重构距离思想的推导过程"></a>基于最小重构距离思想的推导过程</h3><p>最小重构距离指：<br>恢复到原始数据的代价最小(当我们考虑最大投影方差时，其实也可以任务数据尽可能的分散，那么恢复成原始数据的代价就更小，如果数据都重合在了一起，那么想要恢复数据，也就是重构，就更加困难了)</p>
<ol>
<li><p>在新基底(以S的特征向量为基底)的向量表示<br>$\vec{x_{i}} = \sum_{k=1}^{p}(\vec{x_{i}^{T}}\cdot \vec{u_{k}} )\cdot \vec{u_{k}}$<br>为了简化描述，我们先假设数据已经中心化，注意这里的<strong>原始数据是p维的</strong></p>
</li>
<li><p>降至<strong>q维</strong>的向量表示<br>$\vec{\hat{x_{i}}} = \sum_{k=1}^{q}(\vec{x_{i}^{T}}\cdot \vec{u_{k}} )\cdot \vec{u_{k}}$</p>
</li>
<li><p>最小重构代价计算<br>$J = \frac{1}{N} \sum_{i=1}^{N}\parallel \vec{x_{i}}-\hat{x_{i}} \parallel^{2}$</p>
<p>$= \frac{1}{N} \sum_{i=1}^{N}\parallel   \sum_{k=q+1}^{p}(\vec{x_{i}^{T}} \cdot \vec{u_{k}} )\cdot \vec{u_{k}}   \parallel^{2}$</p>
<p>$= \frac{1}{N} \sum_{i=1}^{N} \sum_{k=q+1}^{p}(x_{i}^{T}\cdot u_{k})^{2}$<br>前面我们假设数据已经中心化，现在我们代入真正的数据<br>$J= \frac{1}{N} \sum_{i=1}^{N} \sum_{k=q+1}^{p}\begin{Bmatrix}<br>(x_{i}^{T}-\bar{x})\cdot u_{k}<br>\end{Bmatrix}^{2}$</p>
</li>
</ol>
<p>$J= \sum_{k=q+1}^{p}\sum_{i=1}^{N} \frac{1}{N}  \begin{Bmatrix}<br>(x_{i}^{T}-\bar{x})\cdot u_{k}<br>\end{Bmatrix}^{2}$</p>
<p>$J=\sum_{k=q+1}^{p}u_{k}^{T}\cdot S \cdot u_{k}$<br>s.t. $u_{k}^{T}\cdot u_{k}=1$</p>
<p>到这里我们的推导已经基本上完成了，下面是我们的优化目标：让J最小<br>$J = argmin (\sum_{k=q+1}^{p}u_{k}^{T}\cdot S \cdot u_{k})$<br>s.t. $u_{k}^{T}\cdot u_{k}=1$<br>在这里我们可以发现，这个优化目标与最大方差的优化目标形式是一样的，只不过一个是求最大的$\vec{u}$，一个是降至k维后余下向量的$\vec{u}$<br>要求最小的J的话,应该求最小的特征值，那么剩下的特征值就是前q维的主成分了。</p>
<h2 id="PCA的计算过程-IPO"><a href="#PCA的计算过程-IPO" class="headerlink" title="PCA的计算过程(IPO)"></a>PCA的计算过程(IPO)</h2><p>input:  $X_{N \times p}$,降至d维<br>Process:</p>
<ol>
<li>中心化 $X:= H \cdot X$ </li>
<li>计算样本的协方差矩阵S</li>
<li>对S做特征值分解</li>
<li>取前d大个特征值所对应的特征向量 $u_{1},u_{2},…u_{d}$(列向量)组成投影矩阵$P=(u_{1}u_{2}…u_{d})$</li>
</ol>
<p>Output: $X_{N \times d} =  X_{N \times p} \cdot P_{p \times d}$</p>
<p>这里还有一个小细节,就是西瓜书中计算新的坐标时是采用:<br> $X_{d \times N} = P^{T}_{d \times p} \cdot X_{p \times N}$<br> 最后得出来的结果是一样的.<br> 网上的很多文章对这个都解释的不太清楚,这里从矩阵线性变换的角度来理解看看:<br> 首先,计算一个列向量$x$在新基下面的坐标是这样计算的:<br> $x^{‘}: P^{-1} \cdot x$<br> 而如果将列向量组成矩阵,那么计算应该是这样的:<br> $X_{d \times N} = P^{-1}_{d \times p} \cdot X_{p \times N}$<br> 但是,如果是标准正交基的话,有:<br> $P^{-1} = P^{T}$<br> 这样就变成西瓜书上面的:<br>  $X_{d \times N} = P^{T}_{d \times p} \cdot X_{p \times N}$</p>
<p>关于PCA的内容还有一个是关于PCA的计算优化PCoA，使用SVD分解，以及scikit learn中的PCA源码就是使用PCoA，这个先挖坑待填。</p>
<hr>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a href="https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/" target="_blank" rel="noopener">Vincent Spruyt. The Curse of Dimensionality in classification. Computer vision for dummies. 2014.</a></li>
<li>西瓜书</li>
<li>PRML</li>
<li><a href="https://www.cnblogs.com/yanxingang/p/10776475.html" target="_blank" rel="noopener">求和向量和中心矩阵</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/24709748" target="_blank" rel="noopener">矩阵求导术</a></li>
<li><a href="https://www.bilibili.com/video/BV1aE411o7qd?t=4&amp;p=4" target="_blank" rel="noopener">白板推导系列</a></li>
</ol>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title>NG机器学习EX2(逻辑回归2)</title>
    <url>/2020/11/06/NG%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0EX2-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%922/</url>
    <content><![CDATA[<p>本篇博客的主要内容有：</p>
<ol>
<li>Regularization理论知识</li>
<li>python代码实现EX2的第二个练习</li>
</ol>
<hr>
<a id="more"></a>
<h2 id="Regularization理论知识"><a href="#Regularization理论知识" class="headerlink" title="Regularization理论知识"></a>Regularization理论知识</h2><h3 id="the-problem-of-overfitting"><a href="#the-problem-of-overfitting" class="headerlink" title="the problem of overfitting"></a>the problem of overfitting</h3><p> 在线性回归模型中</p>
<p><img src="/images/4 NG EX2/underOverfit.png" width="50%" height="50%"> </p>
<ul>
<li>第一个模型是一个线性模型,欠拟合(高偏差 high bias),不能很好地适应我们的训练集。它认为房屋的价格会随着size呈线性变化,但是实际上是不会的。</li>
<li>第三个模型是一个四次方的模型,过于强调拟合原始数据,而丢失了算法的本质:预测新数据。若给出一个新的值使之预测,它将表现的很差,是过拟合(高方差 high variance)，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好;</li>
<li>中间的模型似乎最合适。</li>
</ul>
<p>逻辑回归模型中也是同样的问题</p>
<p><img src="/images/4 NG EX2/underOverfit2.png" width="50%" height="50%"></p>
<p><strong>Underfitting,or high bias,</strong> is when the form of our hypothesis function h maps poorly to the trend of the data.<br>It is usually caused by a function that is too simple or uses too few features.</p>
<p><strong>overfitting, or high variance,</strong> is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.</p>
<p>过拟合问题的解决方法:</p>
<ul>
<li><p>Reduce the number of features:<br>Manually select which features to keep.<br>Use a model selection algorithm (比如说PCA).</p>
</li>
<li><p>Regularization<br>Keep all the features, but reduce the magnitude of parameters $\theta_{j}$<br>Regularization works well when we have a lot of slightly useful features.</p>
</li>
</ul>
<p>也就是说,使用Regularization方法,我们可以保留所有的特征(这些特征对模型训练尽管不是起着关键作用,但是也能做出一定的贡献),又同时能<strong>解决过拟合问题</strong></p>
<h3 id="cost-function"><a href="#cost-function" class="headerlink" title="cost function"></a>cost function</h3><p>线性回归中我们发生过拟合的模型是:<br>$h_{\theta}(x) = \theta_{0} + \theta_{1}x + \theta_{2}x^{2} + \theta_{3}x^{3} + \theta_{4}x^{4}$<br>如果我们penalize  $\theta_{3},\theta_{4}$ 并且让他们非常的小,那么我们的拟合曲线就会变得是一个最高次为2的曲线(3,4次由于系数为0将会变得忽略不计)<br><img src="/images/4 NG EX2/smooth.png" width="50%" height="50%"><br>如上图,这样我们的曲线将会更加的光滑,不容易发生过拟合<br>所以如果我们将$\theta_{j}$加入我们的cost function,然后来最小化我们的cost时,就会同时最小化我们的$\theta_{j}$</p>
<p>下面是cost function的重新定义:<br>$J\left( {\theta_{0}},{\theta_{1}}…{\theta_{n}} \right)=\frac{1}{2m} [\sum_{i=1}^{m}(h_{\theta }(x^{(i)})-y^{(i)})^{2} + \lambda \sum_{j=1}^{n}\theta_{j}^{2}]$</p>
<p><strong>根据习惯,我们将从$\theta_{1}$开始惩罚,而不是$\theta_{0}$</strong><br>也就是说,我们在原来的cost上面加了一个$\lambda \sum_{j=1}^{n}\theta_{j}^{2}$(正则项),通过优化cost,我们也会让$\theta_{j}$更小,进而$h_{\theta}(x)$也会更加光滑(可以从前面$\theta_{3},\theta_{4}$的例子上直观感受到)</p>
<p>具体来说,如果我们令 λ 的值很大的话,为了使cost尽可能的小,所有的$\theta$值(不包括$\theta_{0}$)都会在一定程度上减小。<br>但若$\theta$的值太大了所有的 $\theta$值(不包括$\theta_{0}$),都会趋近于0,这样我们所得到的只能是一条平行于x轴的直线($h_{\theta}(x)=\theta_{0}$),这样我们的模型就会有过强的bias,有过高的偏差,认为预测的价格一直都是等于$\theta_{0}$,换句话说,过大的$\lambda$平滑了太多的$h_{\theta}(x)$,导致了欠拟合(underfitting)</p>
<p>所以对于正则化，<strong>我们要取一个合理的$\lambda$的值</strong>，这样才能更好的应用正则化</p>
<p>$\lambda$在这里是用来平衡我们的两个不同目标:</p>
<ul>
<li>让我们的模型能够更好的拟合数据</li>
<li>同时让我们的$\theta_{j}$更小,来平滑$h_{\theta}(x)$,进而避免过拟合</li>
</ul>
<p>$\lambda$是rugularization parameter,它<strong>决定了我们的参数成本将被扩大到多少</strong></p>
<p>也就是说,<strong>我们通过在cost中增加额外的正则项(参数成本),使得$h_{\theta}(x)$变得平滑(减少$h_{\theta}(x)$中某些项的权重),而没有减少特征数目或者改变$h_{\theta}(x)$的形式进而避免过拟合</strong></p>
<h3 id="regularized-logistic-regression"><a href="#regularized-logistic-regression" class="headerlink" title="regularized logistic regression"></a>regularized logistic regression</h3><p>$J(\theta) = \frac{1}{m}\sum_{i=1}^{m}[-y^{i}log(h_{\theta}(X^{(i)}))-(1-y^{i})log(1-h_{\theta}(x^{(i)}))] + \frac{\lambda}{2m} \sum_{j=1}^{n}\theta_{j}^{2}$<br>其梯度定义为:<br>$\frac{\partial J(\theta)}{\partial \theta_{0}} = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)}$</p>
<p>$\frac{\partial J(\theta)}{\partial \theta_{j=1,2…n}} = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)} + \frac{\lambda}{m}\theta_{j}$</p>
<p>那么如果使用梯度下降的话,</p>
<p>Grandient Descent:<br>repeat{<br>    $\theta_{j} := \theta_{j} - \alpha \cdot \frac{\partial J(\theta)}{\partial \theta_{j}} $<br>}<br>依旧是上面这个,但是需要注意$\theta_{0}$是<strong>没有正则化项的</strong></p>
<h2 id="python代码实现"><a href="#python代码实现" class="headerlink" title="python代码实现"></a>python代码实现</h2><p>这部分,我们使用加入正则项提升逻辑回归算法,正则化是cost函数中的一个术语,它使算法更倾向于“更简单”的模型(在这种情况下，模型将更小的系数)。有助于减少过拟合，提高模型的泛化能力。</p>
<blockquote>
<p>设想你是工厂的生产主管，你有一些芯片在两次测试中的测试结果。对于这两次测试，你想决定是否芯片要被接受或抛弃。为了帮助你做出艰难的决定，你拥有过去芯片的测试数据集，从其中你可以构建一个逻辑回归模型</p>
</blockquote>
<h3 id="读入数据并可视化"><a href="#读入数据并可视化" class="headerlink" title="读入数据并可视化"></a>读入数据并可视化</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def plotData(data2):</span><br><span class="line">    positive &#x3D; data2[data2[&#39;Accepted&#39;].isin([1])]</span><br><span class="line">    negative &#x3D; data2[data2[&#39;Accepted&#39;].isin([0])]</span><br><span class="line">    # print(positive)</span><br><span class="line"></span><br><span class="line">    fig,ax &#x3D; plt.subplots(figsize&#x3D;(12,8))</span><br><span class="line">    ax.scatter(positive[&#39;Test 1&#39;],positive[&#39;Test 2&#39;],s&#x3D;100,c&#x3D;&#39;black&#39;,marker&#x3D;&#39;+&#39;,label &#x3D; &#39;Accepted&#39;)</span><br><span class="line">    ax.scatter(negative[&#39;Test 1&#39;], negative[&#39;Test 2&#39;], s&#x3D;50, c&#x3D;&#39;y&#39;, marker&#x3D;&#39;o&#39;, label&#x3D;&#39;Rejected&#39;)</span><br><span class="line">    ax.legend()</span><br><span class="line">    ax.set_xlabel(&#39;Test 1 Score&#39;)</span><br><span class="line">    ax.set_ylabel(&#39;Test 2 Score&#39;)</span><br><span class="line">    #plt.show()</span><br><span class="line">path &#x3D; &#39;ex2data2.txt&#39;</span><br><span class="line">data2 &#x3D; pd.read_csv(path,header&#x3D;None,names&#x3D;[&#39;Test 1&#39;,&#39;Test 2&#39;,&#39;Accepted&#39;])</span><br><span class="line">plotData(data2)</span><br></pre></td></tr></table></figure>
<p><img src="/images/4 NG EX2/data2.png" width="50%" height="50%"><br>这个数据比第一个练习的复杂得多,没有线性决策边界来的分开两类数据</p>
<h3 id="Feature-Mapping"><a href="#Feature-Mapping" class="headerlink" title="Feature Mapping"></a>Feature Mapping</h3><p>如上图所示,如果我们直接使用线性回归是没有办法很好的达到好分类效果的,因为<strong>逻辑回归只能去找到一个线性决策边界</strong><br>有一种解决办法是:create more features from each data point(根据输入的数据创造更多的特征),我们使用一个特征映射函数,将数据映射映射到所有的多项式项中，最高可达到六次幂</p>
<p>$mapFeature(x) = \begin{bmatrix}<br>1<br>\\<br>x_{1}<br>\\<br>x_{2}<br>\\<br>x_{1}^{2}<br>\\<br>x_{1}x_{2}<br>\\<br>x_{2}^{2}<br>\\<br>x_{1}^{3}<br>\\<br>x_{1}^{2}x_{2}<br>\\<br>x_{1}x_{2}^{2}<br>\\<br>x_{2}^{3}<br>\\<br>…<br>\\<br>x_{1}x_{2}^{5}<br>\\<br>x_{2}^{6}<br>\end{bmatrix}<br>$</p>
<p>通过这样的映射,我们的特征向量,原本只有2D,被转换为了28D的向量。这样我们的逻辑回归分类器通过训练更高维的特征向量,将会有更加复杂的决策边界(非线性)<br>下面是特征映射函数的实现:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def mapFeature(x1,x2,degree&#x3D;6):</span><br><span class="line">    # degree &#x3D; 6</span><br><span class="line">    data &#x3D; &#123;&#125;</span><br><span class="line">    for i in range(1,degree+1):</span><br><span class="line">        for j in range(0,i+1):</span><br><span class="line">            # data2[&#39;F&#39;+str(i)+str(j)] &#x3D; np.power(x1,i-j)*np.power(x2,j)</span><br><span class="line">            data[&quot;f&#123;&#125;&#123;&#125;&quot;.format(i - j, j)] &#x3D; np.power(x1, i - j) * np.power(x2, j)</span><br><span class="line">    data &#x3D; pd.DataFrame(data)</span><br><span class="line">    data.insert(0,&#39;Ones&#39;,1) </span><br><span class="line">    return data</span><br><span class="line"></span><br><span class="line">degree &#x3D; 6</span><br><span class="line">x1 &#x3D; data2[&#39;Test 1&#39;]</span><br><span class="line">x2 &#x3D; data2[&#39;Test 2&#39;]</span><br><span class="line">X2 &#x3D; np.array(mapFeature(x1,x2,degree))</span><br><span class="line">y2 &#x3D; np.array(data2.iloc[:,2:3])</span><br><span class="line">theta2 &#x3D; np.zeros(X2.shape[1])</span><br><span class="line">lambdaa &#x3D; 1</span><br><span class="line"># lambdaa &#x3D; 100</span><br><span class="line">print(X2[0:5])</span><br><span class="line">print(y2[0:5])</span><br></pre></td></tr></table></figure><br>通过这个函数,我们的第一个数据将会是:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[ 1.00000000e+00  5.12670000e-02  6.99560000e-01  2.62830529e-03</span><br><span class="line">   3.58643425e-02  4.89384194e-01  1.34745327e-04  1.83865725e-03</span><br><span class="line">   2.50892595e-02  3.42353606e-01  6.90798869e-06  9.42624411e-05</span><br><span class="line">   1.28625106e-03  1.75514423e-02  2.39496889e-01  3.54151856e-07</span><br><span class="line">   4.83255257e-06  6.59422333e-05  8.99809795e-04  1.22782870e-02</span><br><span class="line">   1.67542444e-01  1.81563032e-08  2.47750473e-07  3.38066048e-06</span><br><span class="line">   4.61305487e-05  6.29470940e-04  8.58939846e-03  1.17205992e-01]</span><br></pre></td></tr></table></figure><br>具有28个维度</p>
<h3 id="计算代价函数和梯度"><a href="#计算代价函数和梯度" class="headerlink" title="计算代价函数和梯度"></a>计算代价函数和梯度</h3><p>sigmoid函数在第一个练习中已经实现了,现在我们需要做的,是更新代价函数和梯度,加入正则化项。<br>$J(\theta) = \frac{1}{m}\sum_{i=1}^{m}[-y^{i}log(h_{\theta}(X^{(i)}))-(1-y^{i})log(1-h_{\theta}(x^{(i)}))] + \frac{\lambda}{2m} \sum_{j=1}^{n}\theta_{j}^{2}$<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def costFunction(theta,X,y,lambdaa):</span><br><span class="line"></span><br><span class="line">    h &#x3D; sigmoid(X.dot(theta.T))</span><br><span class="line">    J &#x3D;(- y.T .dot(np.log(h))-(1-y).T.dot(np.log(1-h) ))&#x2F;len(X)</span><br><span class="line">    reg &#x3D; theta[2:].T .dot(theta[2:] )* lambdaa &#x2F; (2*len(X))</span><br><span class="line">    J &#x3D; J + reg</span><br><span class="line">    return J</span><br></pre></td></tr></table></figure></p>
<p>$\frac{\partial J(\theta)}{\partial \theta_{0}} = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)}$</p>
<p>$\frac{\partial J(\theta)}{\partial \theta_{j=1,2…n}} = \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)} + \frac{\lambda}{m}\theta_{j}$</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def gradFunction(theta,X,y,lambdaa):</span><br><span class="line">    grad &#x3D; np.zeros(len(theta))</span><br><span class="line">    theta &#x3D; theta.reshape(1,len(theta))</span><br><span class="line">    h &#x3D; sigmoid(X.dot(theta.T))</span><br><span class="line">    reg &#x3D; lambdaa * theta &#x2F; len(X)</span><br><span class="line">    reg[0][0] &#x3D; 0</span><br><span class="line">    grad &#x3D; X.T.dot ((h-y)) &#x2F; len(X) + reg.T</span><br><span class="line">    return grad</span><br></pre></td></tr></table></figure>
<h3 id="predict"><a href="#predict" class="headerlink" title="predict"></a>predict</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def predict(theta,X):</span><br><span class="line">    probality &#x3D; sigmoid(X.dot(theta.T))</span><br><span class="line">    return [1 if x &gt;&#x3D; 0.5 else 0 for x in probality]</span><br><span class="line"></span><br><span class="line">result2 &#x3D; opt.fmin_tnc(func&#x3D;costFunction, x0&#x3D;theta2, fprime&#x3D;gradFunction, args&#x3D;(X2, y2, lambdaa))</span><br><span class="line">theta_min &#x3D; result2[0]</span><br><span class="line">predictions &#x3D; predict(theta_min,X2)</span><br><span class="line">correct &#x3D; [1 if (a &#x3D;&#x3D; 1 and b &#x3D;&#x3D; 1) or (a &#x3D;&#x3D; 0 and  b &#x3D;&#x3D; 0) else 0 for (a,b) in zip(predictions,y2)]</span><br><span class="line"></span><br><span class="line">accuracy &#x3D; sum(map(int,correct)) * 100 &#x2F;len(correct)</span><br><span class="line">print(&#39;accuracy &#x3D; &#123;0:.2f&#125;%&#39;.format(accuracy))</span><br></pre></td></tr></table></figure>
<p>算出来的结果为:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">accuracy &#x3D; 83.05%</span><br></pre></td></tr></table></figure></p>
<h3 id="decision-boundary"><a href="#decision-boundary" class="headerlink" title="decision boundary"></a>decision boundary</h3><p>下面代码有些不好理解:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#返回num个均匀分布的样本，在[start, stop]</span><br><span class="line">x &#x3D; np.linspace(-1, 1.5, 250)</span><br><span class="line"></span><br><span class="line">#   [X,Y] &#x3D; meshgrid(x,y)</span><br><span class="line"># 将向量x和y定义的区域转换成矩阵X和Y,</span><br><span class="line"># 其中矩阵X的行向量是向量x的简单复制，而矩阵Y的列向量是向量y的简单复制</span><br><span class="line"># 假设x是长度为m的向量，y是长度为n的向量，则最终生成的矩阵X和Y的维度都是 nm（注意不是mn）</span><br><span class="line"></span><br><span class="line">xx, yy &#x3D; np.meshgrid(x, x)</span><br><span class="line"></span><br><span class="line">#下面计算这250个点的z值(也就是等高线的z值,z值相同的连成线,就可以形成边界)</span><br><span class="line">z &#x3D; np.array(mapFeature(xx.ravel(), yy.ravel(), 6))</span><br><span class="line">#上面的xx.ravel()和yy.ravel()都是250*250&#x3D;62500维度的向量,将他们映射为(62500,28)</span><br><span class="line">z &#x3D; z @ theta_min # z是(62500,1)</span><br><span class="line">z &#x3D; z.reshape(xx.shape)</span><br><span class="line">#reshape后,变成了(250,250),这样相当于有250个点的z值</span><br><span class="line"></span><br><span class="line">plotData(data2)</span><br><span class="line"># contour画出等高线</span><br><span class="line">plt.contour(xx, yy, z, 0)</span><br><span class="line">plt.ylim(-.8, 1.2)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><br>画出来的决策边界长下面这样:</p>
<p><img src="/images/4 NG EX2/boundary2.png" width="50%" height="50%"></p>
<p>上图是$\lambda=1$</p>
<p>我们还可以尝试使用不同的$\lambda$来看看效果:</p>
<ul>
<li>$lambda=0$<br><img src="/images/4 NG EX2/lambda=0.png" width="50%" height="50%"></li>
</ul>
<p>这是我们没有正则化的结果(过拟合)</p>
<ul>
<li><p>$lambda=100$<br><img src="/images/4 NG EX2/lambda=100.png" width="50%" height="50%"></p>
</li>
<li><p>$lambda=1000$<br><img src="/images/4 NG EX2/lambda=1000.png" width="50%" height="50%"></p>
</li>
</ul>
<p>当$lambda=100$否则$lambda=1000$ 实际上已经发生了欠拟合。</p>
<h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np </span><br><span class="line">import pandas as pd </span><br><span class="line">import matplotlib.pyplot as plt </span><br><span class="line">import scipy.optimize as opt </span><br><span class="line"></span><br><span class="line">def sigmoid(z):</span><br><span class="line">    return 1&#x2F;(1+np.exp(-z))</span><br><span class="line"></span><br><span class="line">def costFunction(theta,X,y,lambdaa):</span><br><span class="line">    h &#x3D; sigmoid(X.dot(theta.T))</span><br><span class="line">    J &#x3D;(- y.T .dot(np.log(h))-(1-y).T.dot(np.log(1-h) ))&#x2F;len(X)</span><br><span class="line">    reg &#x3D; theta[2:].T .dot(theta[2:] )* lambdaa &#x2F; (2*len(X))</span><br><span class="line">    J &#x3D; J + reg</span><br><span class="line">    return J</span><br><span class="line"></span><br><span class="line">def gradFunction(theta,X,y,lambdaa):</span><br><span class="line">    grad &#x3D; np.zeros(len(theta))</span><br><span class="line">    theta &#x3D; theta.reshape(1,len(theta))</span><br><span class="line">    h &#x3D; sigmoid(X.dot(theta.T))</span><br><span class="line">    reg &#x3D; lambdaa * theta &#x2F; len(X)</span><br><span class="line">    reg[0][0] &#x3D; 0</span><br><span class="line">    grad &#x3D; X.T.dot ((h-y)) &#x2F; len(X) + reg.T</span><br><span class="line">    return grad</span><br><span class="line"></span><br><span class="line">def mapFeature(x1,x2,degree&#x3D;6):</span><br><span class="line">    # degree &#x3D; 6</span><br><span class="line">    data &#x3D; &#123;&#125;</span><br><span class="line">    for i in range(1,degree+1):</span><br><span class="line">        for j in range(0,i+1):</span><br><span class="line">            # data2[&#39;F&#39;+str(i)+str(j)] &#x3D; np.power(x1,i-j)*np.power(x2,j)</span><br><span class="line">            data[&quot;f&#123;&#125;&#123;&#125;&quot;.format(i - j, j)] &#x3D; np.power(x1, i - j) * np.power(x2, j)</span><br><span class="line">    data &#x3D; pd.DataFrame(data)</span><br><span class="line">    data.insert(0,&#39;Ones&#39;,1) </span><br><span class="line">    return data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def predict(theta,X):</span><br><span class="line">    probality &#x3D; sigmoid(X.dot(theta.T))</span><br><span class="line">    return [1 if x &gt;&#x3D; 0.5 else 0 for x in probality]</span><br><span class="line">   </span><br><span class="line">def plotData(data2):</span><br><span class="line">    positive &#x3D; data2[data2[&#39;Accepted&#39;].isin([1])]</span><br><span class="line">    negative &#x3D; data2[data2[&#39;Accepted&#39;].isin([0])]</span><br><span class="line">    # print(positive)</span><br><span class="line"></span><br><span class="line">    fig,ax &#x3D; plt.subplots(figsize&#x3D;(12,8))</span><br><span class="line">    ax.scatter(positive[&#39;Test 1&#39;],positive[&#39;Test 2&#39;],s&#x3D;100,c&#x3D;&#39;black&#39;,marker&#x3D;&#39;+&#39;,label &#x3D; &#39;Accepted&#39;)</span><br><span class="line">    ax.scatter(negative[&#39;Test 1&#39;], negative[&#39;Test 2&#39;], s&#x3D;50, c&#x3D;&#39;y&#39;, marker&#x3D;&#39;o&#39;, label&#x3D;&#39;Rejected&#39;)</span><br><span class="line">    ax.legend()</span><br><span class="line">    ax.set_xlabel(&#39;Test 1 Score&#39;)</span><br><span class="line">    ax.set_ylabel(&#39;Test 2 Score&#39;)</span><br><span class="line">    # plt.show()</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    path &#x3D; &#39;ex2data2.txt&#39;</span><br><span class="line">    data2 &#x3D; pd.read_csv(path,header&#x3D;None,names&#x3D;[&#39;Test 1&#39;,&#39;Test 2&#39;,&#39;Accepted&#39;])</span><br><span class="line">    print(data2.head())</span><br><span class="line">    # plotData(data2)</span><br><span class="line"></span><br><span class="line">    degree &#x3D; 6</span><br><span class="line">    x1 &#x3D; data2[&#39;Test 1&#39;]</span><br><span class="line">    x2 &#x3D; data2[&#39;Test 2&#39;]</span><br><span class="line">    </span><br><span class="line">    X2 &#x3D; np.array(mapFeature(x1,x2))</span><br><span class="line">    y2 &#x3D; np.array(data2.iloc[:,2:3])</span><br><span class="line">    theta2 &#x3D; np.zeros(X2.shape[1])</span><br><span class="line">    lambdaa &#x3D; 1</span><br><span class="line">    #lambdaa &#x3D; 1000</span><br><span class="line"></span><br><span class="line">    #print(costFunction(theta2,X2,y2,lambdaa))</span><br><span class="line">    #print(gradFunction(theta2,X2,y2,lambdaa))</span><br><span class="line">    result2 &#x3D; opt.fmin_tnc(func&#x3D;costFunction, x0&#x3D;theta2, fprime&#x3D;gradFunction, args&#x3D;(X2, y2, lambdaa))</span><br><span class="line"></span><br><span class="line">    theta_min &#x3D; result2[0]</span><br><span class="line">    predictions &#x3D; predict(theta_min,X2)</span><br><span class="line">    correct &#x3D; [1 if (a &#x3D;&#x3D; 1 and b &#x3D;&#x3D; 1) or (a &#x3D;&#x3D; 0 and  b &#x3D;&#x3D; 0) else 0 for (a,b) in zip(predictions,y2)]</span><br><span class="line">    </span><br><span class="line">    accuracy &#x3D; sum(map(int,correct)) * 100 &#x2F;len(correct)</span><br><span class="line">    print(&#39;accuracy &#x3D; &#123;0:.2f&#125;%&#39;.format(accuracy)) </span><br><span class="line"></span><br><span class="line">    x &#x3D; np.linspace(-1, 1.5, 250)</span><br><span class="line">    xx, yy &#x3D; np.meshgrid(x, x)</span><br><span class="line">    z &#x3D; np.array(mapFeature(xx.ravel(), yy.ravel(), 6))</span><br><span class="line">    z &#x3D; z @ theta_min</span><br><span class="line">    z &#x3D; z.reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">    plotData(data2)</span><br><span class="line">    plt.contour(xx, yy, z, 0)</span><br><span class="line">    plt.ylim(-.8, 1.2)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">main()</span><br></pre></td></tr></table></figure>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">黄海广博士的笔记GitHub</a></li>
<li><a href="https://blog.csdn.net/Cowry5/article/details/80247569" target="_blank" rel="noopener">边界绘制参考了这篇csdn博客</a></li>
</ol>
]]></content>
      <categories>
        <category>NG-ML</category>
      </categories>
      <tags>
        <tag>分类</tag>
      </tags>
  </entry>
  <entry>
    <title>NG机器学习EX3(多分类和神经网络)</title>
    <url>/2020/11/10/NG%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0EX3-%E5%A4%9A%E5%88%86%E7%B1%BB%E5%92%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<p>本篇博客的主要内容有：</p>
<ol>
<li>前馈神经网络理论知识</li>
<li>python代码实现EX3的练习(逻辑回归多分类器构建,前馈神经网络的实现)</li>
</ol>
<hr>
<a id="more"></a>
<p>这篇博客的主要来源是吴恩达老师的机器学习第四周的学习课程,还有一些内容参考自<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">黄海广博士的笔记GitHub</a>,先根据作业完成了matlab代码,并且学习将它用python来实现(博客中无matlab实现)。在学习的过程中我结合吴恩达老师的上课内容和matlab代码修改了一部分代码(个人认为可以优化的地方)。</p>
<h2 id="Neural-Networks-Representation"><a href="#Neural-Networks-Representation" class="headerlink" title="Neural Networks: Representation"></a>Neural Networks: Representation</h2><h3 id="Non-linear-Hypotheses"><a href="#Non-linear-Hypotheses" class="headerlink" title="Non-linear Hypotheses"></a>Non-linear Hypotheses</h3><p>线性回归和逻辑回归都有这样一个缺点:<strong>当特征太多时，计算的负荷会非常大。</strong><br>如果对逻辑回归印象模糊的话,需要去看看吴恩达老师前面的课程,因为它是神经网络的基础。<br>在前面的练习过程(EX3)中,我们通过添加多项式项来作为特征来训练,构造非线性分类器来预测。<br>但是,如果我们有非常多的特征,那么构造非线性多项式的分类器会产生非常多的特征组合。<br>例如: 我们有$n$个特征,即使我们使用两两相乘作为特征组合($x_{1}x_{2},x_{1}x_{3},…x_{99}x_{100}$),那么我们会组成$C_{n}^{2}\approx 5000$个特征,<strong>数量过多的特征不仅可能导致过拟合,并且计算代价也是相当大的</strong><br>因此,<strong>用Quadratic features来找Non-linear boundary不是一个好办法</strong><br>也就是说,普通的逻辑回归模型,不能有效地处理这么多的特征，这时候我们就可以考虑使用神经网络了。</p>
<h3 id="Model-Representation"><a href="#Model-Representation" class="headerlink" title="Model Representation"></a>Model Representation</h3><p>我们大脑的神经网络一般是这样的:<br>神经元含有许多的树突(输入),神经元处理计算后,通过轴突(输出)传给其他的神经元,这些神经元共同组成了大脑的神经网络。</p>
<p>在机器学习中,我们将使用<strong>逻辑回归模型</strong>作为我们的神经元模型:<strong>(Logistic Unit)</strong> (activation unit or 激活单元)来构建神经网络,如下图所示:<br><img src="/images/6 NG EX3/logistic unit.png" width="50%" height="50%"><br>这与我们之前学过的逻辑回归模型是一样的,用它构建的神经网络可以是下面这样的:<br><img src="/images/6 NG EX3/network.png" width="50%" height="50%"><br>用逻辑单元构建的神经网络,每一层的输出作为下一层的输入。上图为是一个3层的神经网络,<br>第一层是输入层 <strong>(Input Layer)</strong> ,<br>最后一层是输出层<strong>(Output Layer)</strong> ,<br>除了输入输出层之外,我们统称为隐藏层<strong>(Hidden Layers)</strong>。<br>注意,这里我们我们为每一层都增加一个<strong>bias unit</strong>,就好像是在线性回顾和逻辑回归中的$\theta_{0}$一样</p>
<p>为了更好的表示神经网络模型和向量化,我们使用下面的表示:<br>$a_{i}^{(j)}$ = “activation” of unit i in layer j<br>即: 第j层的第i个激活单元</p>
<p>$\Theta ^{(j)}$ = matrix of weights controling function mapping from layer j to layer j+1<br>即 用一个$\Theta$矩阵来表示:控制从第j层到底j+1层映射的权重矩阵<br>即 $\Theta^{(1)}$表示从第1层映射到第二层的权重矩阵<br>$\Theta_{1i}^{(1)} (i从0到n)$表示从第1层映射到第2层的$a_{1}^{(2)}$的权重,也就是单个逻辑回归单元的$\theta_{0}\sim  \theta_{3}$<br>因此,如果网络的第$j$层有$s_{j}$个单元,在第$j+1$层有$s_{j+1}$个单元,那么$\Theta$的维度是$s_{j+1} \times (s_{j} + 1)$</p>
<p>之所以要$+1$是对每个$a_{1}^{(2)},a_{2}^{(2)},a_{3}^{(2)}$ 添加一个bias unit(可以认为是$\theta_{0}$)</p>
<h3 id="foward-propagation"><a href="#foward-propagation" class="headerlink" title="foward propagation"></a>foward propagation</h3><p>下面我们就进行神经网络的一层一层(从输入到输出)传播的计算(也就是前馈神经网络),为了更好的进行代码实现,我们<strong>需要进行向量化表示</strong>,它不仅能够大量减少我们的代码量,还可以让我们的程序使用计算库来计算(这些计算库可能会有做矩阵运算的优化,进行并行计算等等)<br><img src="/images/6 NG EX3/forward propagation.png" width="50%" height="50%"></p>
<p>上图为前馈神经网络的计算过程,我们设数据为<br>$x = \begin{bmatrix}<br>x_{0}<br>\\<br>x_{1}<br>\\<br>x_{2}<br>\\<br>x_{3}<br>\end{bmatrix}$<br>$z^{(2)} = \begin{bmatrix}<br>z_{1}^{(2)}<br>\\<br>z_{2}^{(2)}<br>\\<br>z_{3}^{(2)}<br>\end{bmatrix}$</p>
<p>$z^{(2)} = \Theta^{(1)} x = \Theta^{(1)} x a^{(1)}$<br>经过sigmoid函数计算后:<br>$a^{(2)} = g(z^{(2)})$<br>这里都是使用矩阵统一向量化计算的,不需要单独求$a_{1}^{(2)},a_{2}^{(2)},a_{3}^{(2)}$<br>得到$a^{(2)}$之后,我们还需要添加 $a_{0}^{(2)} = 1$<br>所以 神经网络就像是逻辑回归,只是我们把逻辑回归中的输入向量$[x_{1},x_{2},x_{3}]$ 变成了中间层的$[a_{1}^{(2)},a_{2}^{(2)},a_{3}^{(2)}]$,然后我们需要添加逻辑回归中的$\theta_{0}$<br>但是和逻辑回归不同的是,逻辑回归受输入的原始特征$[x_{1},x_{2},x_{3}]$的限制,即使我们用多项式来组合这些特征,我们依旧受原始特征的限制,当神经网络不同,神经网络能从输入中学习自己的特征$[a_{1}^{(2)},a_{2}^{(2)},a_{3}^{(2)}]$,再将它输入逻辑回归单元中,神经网络具有更强的灵活性</p>
<p>下面是前馈神经网络的计算总结:<br>$z^{(j)} = \Theta^{(j-1)} a^{(j-1)}$<br>$a^{(j)} = g(z^{(j)})$<br>$z^{(j+1)} = \Theta^{(j)} a^{(j)}$<br>$h_{\theta}(x) = a^{(j+1)} = g(z^{(j+1)})$ </p>
<p><strong>神经网络能够构造更加复杂的函数,不断学习新的特征,新的更复杂的特征,进而不断层次深入,让我们的逻辑回归分类器能够做出更准确的预测</strong>,下面是两个例子对这句话的直观理解</p>
<h3 id="Eaxmples"><a href="#Eaxmples" class="headerlink" title="Eaxmples"></a>Eaxmples</h3><ol>
<li><p>线性可分 实现AND逻辑运算<br>and运算是线性可分的,可以从下图来看:<br><img src="/images/6 NG EX3/and.png" width="50%" height="50%"><br>通过构造下面的神经网络模型,我们就可以让$h_{\theta}(x) \approx x_{1} and x_{2}$<br><img src="/images/6 NG EX3/andh.png" width="50%" height="50%"></p>
</li>
<li><p>非线性可分 实现异或非(XNOR)<br>异或非的数据是这样子的(非线性可分):<br><img src="/images/6 NG EX3/xnor.png" width="50%" height="50%"><br>吴恩达老师通过联合多个逻辑单元,使得网络可以让$h_{\theta}(x) \approx x_{1} xnor x_{2}$<br><img src="/images/6 NG EX3/xnorh.png" width="50%" height="50%"></p>
</li>
</ol>
<p>至于如何训练这些..后面的课程应该会说明</p>
<h2 id="python代码实现"><a href="#python代码实现" class="headerlink" title="python代码实现"></a>python代码实现</h2><h3 id="使用逻辑回归实现多分类"><a href="#使用逻辑回归实现多分类" class="headerlink" title="使用逻辑回归实现多分类"></a>使用逻辑回归实现多分类</h3><h4 id="读入数据并可视化"><a href="#读入数据并可视化" class="headerlink" title="读入数据并可视化"></a>读入数据并可视化</h4><p>由于作业中的数据集是.mat格式的,我们需要使用使用一个SciPy中的loadmat来加载它<br>数据是手写数字的图片,是5000<em>400的,表示有5000张图片,然后400表示的是20</em>20的图片,将灰度图像转为一个行向量就变成了400</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from scipy.io import loadmat</span><br><span class="line">from scipy.optimize import minimize</span><br></pre></td></tr></table></figure>
<p>下面是可视化数据,参考了<a href="https://blog.csdn.net/qq_45604750/article/details/107628153" target="_blank" rel="noopener">这篇博客</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def displayData(X):</span><br><span class="line">    fig, ax &#x3D; plt.subplots(nrows&#x3D;10, ncols&#x3D;10, sharey&#x3D;True, sharex&#x3D;True)       # 生成10行10列的画布，坐标轴共享</span><br><span class="line">    imageIndex &#x3D; np.random.randint(0, 5000, (100, )) # 返回100个随机整型数作为图片序号</span><br><span class="line">    for row in range(10):</span><br><span class="line">        for col in range(10):</span><br><span class="line">            ax[row, col].matshow(X[imageIndex[10 * row + col]].reshape((20, 20)), cmap&#x3D;&#39;gray_r&#39;)    # 每个图上展示一个灰度数字</span><br><span class="line"></span><br><span class="line">    plt.xticks([])                         # 去除坐标</span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">data &#x3D; loadmat(&#39;ex3data1.mat&#39;)</span><br><span class="line">X &#x3D; np.array(data[&#39;X&#39;])  # 5000 * 400</span><br><span class="line">y &#x3D; np.array(data[&#39;y&#39;])  # 5000 * 1</span><br><span class="line">displayData(X)</span><br></pre></td></tr></table></figure>
<p>可视化结果如下图:<br><img src="/images/6 NG EX3/data.png" width="50%" height="50%"></p>
<h4 id="sigmoid-cost-grad函数"><a href="#sigmoid-cost-grad函数" class="headerlink" title="sigmoid cost grad函数"></a>sigmoid cost grad函数</h4><p>这三个函数和上次逻辑回归的作业添加正则项的没有差别,因此这里不再多说明了<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def sigmoid(z):</span><br><span class="line">    return 1 &#x2F; (1 + np.exp(-z))</span><br><span class="line"></span><br><span class="line">def lrCostFunction(theta,X,y,lambdaa):</span><br><span class="line">    h &#x3D; sigmoid(X.dot(theta.T))</span><br><span class="line">    J &#x3D;(- y.T .dot(np.log(h))-(1-y).T.dot(np.log(1-h) ))&#x2F;len(X)</span><br><span class="line">    reg &#x3D; theta[2:].T .dot(theta[2:] )* lambdaa &#x2F; (2*len(X))</span><br><span class="line">    J &#x3D; J + reg</span><br><span class="line">    return J</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def lrGradFunction(theta,X,y,lambdaa):</span><br><span class="line">    grad &#x3D; np.zeros(len(theta))</span><br><span class="line">    theta &#x3D; theta.reshape(1,len(theta))</span><br><span class="line">    h &#x3D; sigmoid(X.dot(theta.T))</span><br><span class="line">    reg &#x3D; lambdaa * theta &#x2F; len(X)</span><br><span class="line">    reg[0][0] &#x3D; 0</span><br><span class="line">    grad &#x3D; X.T.dot ((h-y)) &#x2F; len(X) + reg.T</span><br><span class="line">    return grad</span><br></pre></td></tr></table></figure></p>
<h4 id="one-vs-all函数"><a href="#one-vs-all函数" class="headerlink" title="one_vs_all函数"></a>one_vs_all函数</h4><p>下面我们定义一对多函数来实现多分类<br>实现多分类的思想是:<br>对于多分类问题,我们可以每次将一类样本当作一类,其他所有样本当作另外一类,c类样本的话总共需要c个分类器,当我们用来预测时,只需要将新的样本输入,然后选择c个分类器中概率最大的那个<br>下面是具体的代码实现:<br>这里面有几个地方需要注意下:</p>
<ul>
<li>我们将X数据插入1矩阵,从5000<em>400变为5000</em>501</li>
<li>对于c个分类器,我们训练的时候,如果样本是属于第$w$类,那么我们就属于$w$类的y标记为1,其他所有的样本标记为0</li>
<li>使用SciPy的fmin函数来优化cost</li>
<li>计算过程中注意维度的不同,程序报错一般情况下是数据维度不一致导致不能相乘<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def one_vs_all(X,y,num_labels,lambdaa):</span><br><span class="line">    </span><br><span class="line">    m &#x3D; X.shape[0]</span><br><span class="line">    n &#x3D; X.shape[1]</span><br><span class="line">    X &#x3D; np.insert(X, 0, values&#x3D;np.ones(m), axis&#x3D;1) # X:5000 * 401</span><br><span class="line"></span><br><span class="line">    all_theta &#x3D; np.zeros((num_labels,n+1))</span><br><span class="line">    for c in range(1,num_labels+1):</span><br><span class="line">        initial_theta &#x3D; np.zeros((n + 1, 1))</span><br><span class="line">        binary_y &#x3D;  np.array([1 if label &#x3D;&#x3D; c else 0 for label in y])</span><br><span class="line">        binary_y &#x3D; np.reshape(binary_y, (m, 1))</span><br><span class="line">        fmin &#x3D; minimize(fun&#x3D;lrCostFunction, x0&#x3D;initial_theta, args&#x3D;(X, binary_y, lambdaa), method&#x3D;&#39;TNC&#39;, jac&#x3D;lrGradFunction)</span><br><span class="line">        all_theta[c-1,:] &#x3D; fmin.x</span><br><span class="line">    return all_theta  # 10 * 401</span><br></pre></td></tr></table></figure>
接下来我们定义预测函数</li>
</ul>
<h4 id="predictOneVsAll"><a href="#predictOneVsAll" class="headerlink" title="predictOneVsAll"></a>predictOneVsAll</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def predictOneVsAll(X, all_theta):</span><br><span class="line">    m &#x3D; X.shape[0]</span><br><span class="line">    X &#x3D; np.insert(X, 0, values&#x3D;np.ones(m), axis&#x3D;1) # 5000 * 401</span><br><span class="line">    h &#x3D; sigmoid(X @ all_theta.T) #5000 * 10</span><br><span class="line">    h_argmax &#x3D; np.argmax(h,axis&#x3D;1)</span><br><span class="line">    return h_argmax + 1</span><br></pre></td></tr></table></figure>
<p>这里函数返回的是h_argmax + 1,是因为,我们的类别预测返回的索引是从0开始的,但是我们的y是从1-10,并且0类对应的是第10类<br>接下里就可以计算模型的准确率了,构建数据调用函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">num_labels &#x3D; len(np.unique(y))</span><br><span class="line">lambdaa &#x3D; 0.1</span><br><span class="line">m &#x3D; X.shape[0]</span><br><span class="line">n &#x3D; X.shape[1]</span><br><span class="line"></span><br><span class="line">all_theta &#x3D; np.zeros((num_labels,n+1)) # 10 * 401的matrix</span><br><span class="line">all_theta &#x3D; one_vs_all(X,y,num_labels,lambdaa)</span><br><span class="line">y_pred &#x3D; predictOneVsAll(X,all_theta)</span><br><span class="line">correct &#x3D; [1 if a &#x3D;&#x3D; b else 0 for (a, b) in zip(y_pred, y)]</span><br><span class="line">accuracy &#x3D; (sum(map(int, correct)) &#x2F; len(correct))</span><br><span class="line">print (&#39;accuracy &#x3D; &#123;:.2f&#125;%&#39;.format(accuracy * 100))</span><br></pre></td></tr></table></figure>
<p>可以得到:<br>$\lambda = 0.1:$<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">accuracy &#x3D; 96.46%</span><br></pre></td></tr></table></figure></p>
<p>接下来是前馈神经网络的实现</p>
<h4 id="foward-propagation-1"><a href="#foward-propagation-1" class="headerlink" title="foward propagation"></a>foward propagation</h4><p>实现的时候注意矩阵维度,其他就是更加前面的传播总结写的代码<br>注意这里的数据$\Theta^{(1)},\Theta^{(2)}$都是作业给出的已经计算好的权重矩阵,这里的话直接使用即可<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Theta1 has size 25 x 401</span><br><span class="line"># Theta2 has size 10 x 26</span><br><span class="line">def predictFeedforwardPropagation(X,Theta1,Theta2):</span><br><span class="line">    m &#x3D; X.shape[0]</span><br><span class="line">    num_labels &#x3D; Theta2.shape[0]</span><br><span class="line">    X &#x3D; np.insert(X, 0, values&#x3D;np.ones(m), axis&#x3D;1)</span><br><span class="line">    z2 &#x3D; Theta1 @ X.T</span><br><span class="line">    a2 &#x3D; sigmoid(z2) # 25 * 5000</span><br><span class="line">    print(a2.shape)</span><br><span class="line">    a2 &#x3D; np.insert(a2, 0, values&#x3D;np.ones((1,X.shape[0])), axis&#x3D;0)</span><br><span class="line">    print(a2.shape) # 26 * 5000</span><br><span class="line">    z3 &#x3D; Theta2 @ a2 </span><br><span class="line">    h &#x3D; sigmoid(z3) # 10 * 5000</span><br><span class="line">    # print(h.shape) </span><br><span class="line">    h_argmax &#x3D; np.argmax(h,axis&#x3D;0)</span><br><span class="line">    # print(h_argmax.shape) # 5000,</span><br><span class="line">    return h_argmax + 1</span><br></pre></td></tr></table></figure><br>接下来使用神经网络分类:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data2 &#x3D; loadmat(&#39;ex3weights.mat&#39;)</span><br><span class="line"></span><br><span class="line">Theta1 &#x3D; np.array(data2[&#39;Theta1&#39;])</span><br><span class="line">Theta2 &#x3D; np.array(data2[&#39;Theta2&#39;])</span><br><span class="line">y_feed_pred &#x3D; predictFeedforwardPropagation(X,Theta1,Theta2)</span><br><span class="line">feed_correct &#x3D; [1 if a &#x3D;&#x3D; b else 0 for (a, b) in zip(y_feed_pred, y)]</span><br><span class="line">feed_accuracy &#x3D; (sum(map(int, feed_correct)) &#x2F; len(feed_correct))</span><br><span class="line">print (&#39;accuracy &#x3D; &#123;:.2f&#125;%&#39;.format(feed_accuracy * 100)) # 97.52%</span><br></pre></td></tr></table></figure><br>最后的准确率可以达到:<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">accuracy &#x3D; 97.52%</span><br></pre></td></tr></table></figure></p>
<h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from scipy.io import loadmat</span><br><span class="line">from scipy.optimize import minimize</span><br><span class="line"></span><br><span class="line">def displayData(X):</span><br><span class="line">    fig, ax &#x3D; plt.subplots(nrows&#x3D;10, ncols&#x3D;10, sharey&#x3D;True, sharex&#x3D;True)       # 生成10行10列的画布，坐标轴共享</span><br><span class="line">    imageIndex &#x3D; np.random.randint(0, 5000, (100, )) # 返回100个随机整型数作为图片序号</span><br><span class="line">    for row in range(10):</span><br><span class="line">        for col in range(10):</span><br><span class="line">            ax[row, col].matshow(X[imageIndex[10 * row + col]].reshape((20, 20)), cmap&#x3D;&#39;gray_r&#39;)    # 每个图上展示一个灰度数字</span><br><span class="line"></span><br><span class="line">    plt.xticks([])                         # 去除坐标</span><br><span class="line">    plt.yticks([])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">def sigmoid(z):</span><br><span class="line">    return 1 &#x2F; (1 + np.exp(-z))</span><br><span class="line"></span><br><span class="line">def lrCostFunction(theta,X,y,lambdaa):</span><br><span class="line"></span><br><span class="line">    # print(X.shape,theta.shape)</span><br><span class="line">    h &#x3D; sigmoid(X.dot(theta.T))</span><br><span class="line">    J &#x3D;(- y.T .dot(np.log(h))-(1-y).T.dot(np.log(1-h) ))&#x2F;len(X)</span><br><span class="line">    reg &#x3D; theta[2:].T .dot(theta[2:] )* lambdaa &#x2F; (2*len(X))</span><br><span class="line">    J &#x3D; J + reg</span><br><span class="line">    return J</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def lrGradFunction(theta,X,y,lambdaa):</span><br><span class="line">    grad &#x3D; np.zeros(len(theta))</span><br><span class="line">    theta &#x3D; theta.reshape(1,len(theta))</span><br><span class="line">    h &#x3D; sigmoid(X.dot(theta.T))</span><br><span class="line">    reg &#x3D; lambdaa * theta &#x2F; len(X)</span><br><span class="line">    reg[0][0] &#x3D; 0</span><br><span class="line">    grad &#x3D; X.T.dot ((h-y)) &#x2F; len(X) + reg.T</span><br><span class="line">    return grad</span><br><span class="line"></span><br><span class="line">def one_vs_all(X,y,num_labels,lambdaa):</span><br><span class="line">    </span><br><span class="line">    m &#x3D; X.shape[0]</span><br><span class="line">    n &#x3D; X.shape[1]</span><br><span class="line">    # print(&#39;n&#x3D;&#39;,n)</span><br><span class="line">    X &#x3D; np.insert(X, 0, values&#x3D;np.ones(m), axis&#x3D;1) # 5000 * 401</span><br><span class="line"></span><br><span class="line">    all_theta &#x3D; np.zeros((num_labels,n+1))</span><br><span class="line">    for c in range(1,num_labels+1):</span><br><span class="line">    # for c in range(1,2):</span><br><span class="line">        initial_theta &#x3D; np.zeros((n + 1, 1))</span><br><span class="line">        # print( &#39;init&#39;,initial_theta.shape)</span><br><span class="line">        binary_y &#x3D;  np.array([1 if label &#x3D;&#x3D; c else 0 for label in y])</span><br><span class="line">        # print(binary_y.shape)</span><br><span class="line">        binary_y &#x3D; np.reshape(binary_y, (m, 1))</span><br><span class="line">        # print(binary_y.shape)</span><br><span class="line">        fmin &#x3D; minimize(fun&#x3D;lrCostFunction, x0&#x3D;initial_theta, args&#x3D;(X, binary_y, lambdaa), method&#x3D;&#39;TNC&#39;, jac&#x3D;lrGradFunction)</span><br><span class="line">        # print(fmin.x.shape)</span><br><span class="line">        all_theta[c-1,:] &#x3D; fmin.x</span><br><span class="line">    return all_theta # 10 * 401</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def predictOneVsAll(X, all_theta):</span><br><span class="line">    m &#x3D; X.shape[0]</span><br><span class="line">    X &#x3D; np.insert(X, 0, values&#x3D;np.ones(m), axis&#x3D;1) # 5000 * 401</span><br><span class="line">    # print(X.shape,all_theta.T.shape)</span><br><span class="line">    h &#x3D; sigmoid(X @ all_theta.T) #5000 * 10</span><br><span class="line">    h_argmax &#x3D; np.argmax(h,axis&#x3D;1)</span><br><span class="line">    return h_argmax + 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Theta1 has size 25 x 401</span><br><span class="line"># Theta2 has size 10 x 26</span><br><span class="line">def predictFeedforwardPropagation(X,Theta1,Theta2):</span><br><span class="line">    m &#x3D; X.shape[0]</span><br><span class="line">    num_labels &#x3D; Theta2.shape[0]</span><br><span class="line">    X &#x3D; np.insert(X, 0, values&#x3D;np.ones(m), axis&#x3D;1)</span><br><span class="line">    z2 &#x3D; Theta1 @ X.T</span><br><span class="line">    a2 &#x3D; sigmoid(z2) # 25 * 5000</span><br><span class="line">    # print(a2.shape)</span><br><span class="line">    a2 &#x3D; np.insert(a2, 0, values&#x3D;np.ones((1,X.shape[0])), axis&#x3D;0)</span><br><span class="line">    # print(a2.shape) # 26 * 5000</span><br><span class="line">    z3 &#x3D; Theta2 @ a2 </span><br><span class="line">    h &#x3D; sigmoid(z3) # 10 * 5000</span><br><span class="line">    # print(h.shape) </span><br><span class="line">    h_argmax &#x3D; np.argmax(h,axis&#x3D;0)</span><br><span class="line">    # print(h_argmax.shape) # 5000,</span><br><span class="line">    return h_argmax + 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line"></span><br><span class="line">    data &#x3D; loadmat(&#39;ex3data1.mat&#39;)</span><br><span class="line">    X &#x3D; np.array(data[&#39;X&#39;])  # 5000 * 400</span><br><span class="line">    y &#x3D; np.array(data[&#39;y&#39;])  # 5000 * 1 </span><br><span class="line">    # displayData(X)</span><br><span class="line">    num_labels &#x3D; len(np.unique(y))</span><br><span class="line">    lambdaa &#x3D; 0.1</span><br><span class="line">    m &#x3D; X.shape[0]</span><br><span class="line">    n &#x3D; X.shape[1]</span><br><span class="line">    </span><br><span class="line">    all_theta &#x3D; np.zeros((num_labels,n+1)) # 10 * 401的matrix</span><br><span class="line">    all_theta &#x3D; one_vs_all(X,y,num_labels,lambdaa)</span><br><span class="line">    y_pred &#x3D; predictOneVsAll(X,all_theta)</span><br><span class="line">    correct &#x3D; [1 if a &#x3D;&#x3D; b else 0 for (a, b) in zip(y_pred, y)]</span><br><span class="line">    accuracy &#x3D; (sum(map(int, correct)) &#x2F; len(correct))</span><br><span class="line">    print (&#39;one vs all:accuracy &#x3D; &#123;:.2f&#125;%&#39;.format(accuracy * 100))</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    data2 &#x3D; loadmat(&#39;ex3weights.mat&#39;)</span><br><span class="line">    Theta1 &#x3D; np.array(data2[&#39;Theta1&#39;])</span><br><span class="line">    Theta2 &#x3D; np.array(data2[&#39;Theta2&#39;])</span><br><span class="line">    y_feed_pred &#x3D; predictFeedforwardPropagation(X,Theta1,Theta2)</span><br><span class="line">    feed_correct &#x3D; [1 if a &#x3D;&#x3D; b else 0 for (a, b) in zip(y_feed_pred, y)]</span><br><span class="line">    feed_accuracy &#x3D; (sum(map(int, feed_correct)) &#x2F; len(feed_correct))</span><br><span class="line">    print (&#39;NN:accuracy &#x3D; &#123;:.2f&#125;%&#39;.format(feed_accuracy * 100)) # 97.52%</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">黄海广博士的笔记GitHub</a></li>
<li><a href="https://blog.csdn.net/qq_45604750/article/details/107628153" target="_blank" rel="noopener">画出手写数据绘制参考了这篇csdn博客</a></li>
</ol>
]]></content>
      <categories>
        <category>NG-ML</category>
      </categories>
      <tags>
        <tag>分类</tag>
      </tags>
  </entry>
</search>
